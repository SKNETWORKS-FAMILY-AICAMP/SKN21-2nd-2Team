### Spotify 이탈 예측 – 모델 성능 상승과정 요약

#### 1단계: 초기 Baseline (원본 스냅샷 데이터, 수치형만)
- **데이터**: `data/raw_data.csv`
- **피처**: 수치형 6개 (`age`, `listening_time`, `songs_played_per_day`, `skip_rate`, `ads_listened_per_week`, `offline_listening`)
- **모델**: Logistic / 간단 RF (notebooks `FE_validation.ipynb` 초기 실험)
- **모델 사용 이유**
  - Logistic: 가장 단순한 선형 모델로, **“이 데이터로 어느 정도까지가 이론적인 최소 성능인지”** 빠르게 확인하기 위함.
  - RF: 전처리 부담이 적고 비선형 관계를 잡을 수 있어, **초기 트리 계열 baseline**으로 선택.
- **대표 성능**
  - F1 ≈ **0.33**
  - AUC ≈ **0.49**
- **해석**: 원본 스냅샷 + 기본 수치형만으로는 이탈 패턴을 거의 못 잡는 수준.

---

#### 2단계: FE 추가 후 검증 (하지만 큰 이득 없음)
- **시도한 작업**
  - 다양한 FE(engagement_score, songs_per_minute, skip_intensity, ads_pressure 등) 추가
  - 범주형 원-핫, 교호작용, SMOTE + XGB, 앙상블까지 실험
- **최선 조합 (수치형 6 + 기본 FE 4, RF 기준)**
  - F1 ≈ **0.41 ~ 0.42**
  - AUC ≈ **0.53 ~ 0.54**
- **결론**
  - FE를 복잡하게 늘려도 **F1 0.41대 / AUC 0.53대에서 정체**.
  - 범주형, SMOTE, XGB, Voting 앙상블 모두 큰 개선 없이 오히려 과적합/성능 하락.
  - → **현재 스냅샷 구조 자체로는 성능 한계가 명확**함을 확인.
- **모델 사용 이유**
  - RF를 계속 기준으로 사용한 이유:  
    - 전처리/FE 조합만 바꿔도 성능 변화를 안정적으로 비교할 수 있는 **견고한 기준 모델**이기 때문.
  - XGB·Voting 등을 잠깐 쓴 이유는,  
    - “복잡한 모델로도 안 올라가면, 진짜로 **데이터 신호 자체가 약하다**”는 걸 검증하기 위함.

---

#### 3단계: FE 대부분 삭제, “시계열 + 고객접점” 피처 추가
- **새 데이터**: `data/enhanced_data.csv` → `data/enhanced_data_not_clean_FE_delete.csv`
- **추가 피처**
  - 시계열 5개: `listening_time_trend_7d`, `login_frequency_30d`, `days_since_last_login`, `skip_rate_increase_7d`, `freq_of_use_trend_14d`
  - 고객접점 4개: `customer_support_contact`, `payment_failure_count`, `promotional_email_click`, `app_crash_count_30d`
- **RF 기준 시나리오 비교** (same 설정, `feature_engineering_advanced.ipynb`)
  - Baseline (기존 11피처: 수치형 + FE)  
    - F1 ≈ **0.4153**, AUC ≈ **0.5352**
  - Baseline + 시계열(5) → 총 16피처  
    - F1 ≈ **0.4935**, AUC ≈ **0.7278**
  - Baseline + 시계열(5) + 고객접점(4) → 총 20피처  
    - F1 ≈ **0.6246**, AUC ≈ **0.7938**
- **결론**
  - 단순 FE/모델 튜닝이 아니라, **데이터 구조(시계열·고객접점) 자체를 바꾸면서 성능이 한 번에 점프**.
  - F1 기준: **0.41 → 0.62(+0.21)**, AUC 기준: **0.54 → 0.79(+0.25)** 수준 향상.
- **모델 사용 이유**
  - 이 단계에서도 RF를 유지한 이유:
    - **모델은 고정하고, 피처만 바꿨을 때 성능이 얼마나 뛰는지**를 명확하게 보여주기 위해서.
    - 즉, 성능 향상이 “알고리즘 변경”이 아니라 **“데이터(시계열 + 고객접점) 추가 덕분”**이라는 걸 증명하기 위함.

---

#### 4단계: 전체 모델 고도화 + 6피처 LGBM 시뮬레이터(단조제약 앙상블)

##### 4-1. 전체 풀 피처 모델 (서비스 메인용)
- **데이터**: `enhanced_data_not_clean_FE_delete.csv` (시계열 + 고객접점 포함)
- **모델**: HistGradientBoosting (HGB, `models/metrics.json` 참조)
- **대표 실험 결과 예시**
  - HGB 풀 모델: F1 ≈ **0.634**, AUC ≈ **0.817**
- **요약**: 목표였던 “F1 0.6+, AUC 0.8+” 구간에 안정적으로 진입했고, **여러 모델 중 HGB가 가장 좋은 균형 성능을 보여 최종 메인 모델로 선택**.
- **모델 사용 이유**
  - **HistGradientBoosting(HGB)**  
    - 사이킷런 내장 gradient boosting으로, **속도·성능·구현 난이도 밸런스가 좋고**, 파이프라인 연동도 쉬우며,
      동일 데이터·피처 조건에서 다른 모델들보다 **F1/AUC가 가장 높게 나와 서비스 메인 모델로 채택**.

##### 4-2. 6개 피처 LGBM 단조제약 모델 (관리자 시뮬레이터용 서브 모델)
- **사용 피처 (6개)**  
  - `app_crash_count_30d`, `skip_rate_increase_7d`, `days_since_last_login`,  
    `listening_time_trend_7d`, `freq_of_use_trend_14d`, `login_frequency_30d`
- **v1 단일 모델 (`train_simulator_6feat_lgbm_mono_v1_baseline.py`)**
  - Best F1 ≈ **0.526**
  - AUC ≈ **0.7400**
- **v2 앙상블 + Early Stopping (`train_simulator_6feat_lgbm_mono.py`)**
  - 5개 LGBM 앙상블, 단조 제약 유지, `scale_pos_weight` 조정
  - Best F1 ≈ **0.5253**
  - AUC ≈ **0.7388**
  - FP 192 → 170 (약 **11% 감소**, 오탐 줄이면서 안정성↑)
- **역할 분리**
  - **풀 피처 HGB/XGB**: 서비스 전체 이탈 예측 메인 모델  
  - **6피처 LGBM 단조제약 앙상블**: 관리자 화면에서 “크래시/잠수/사용량 추세”를 조정해 보는 **What-if 시뮬레이터 엔진**
- **모델 사용 이유**
  - **LightGBM + 단조 제약(monotone constraints)**  
    - 6개 피처 각각에 대해 “값이 커질수록 위험 ↑ / ↓” 관계를 강제로 지켜 주어서,  
      관리자 입장에서 **입력 값을 조정했을 때 항상 직관적인 방향으로 예측이 변하도록** 만들기 위함.
  - **앙상블(5개 LGBM 평균)**  
    - 단일 모델보다 분산을 줄여, **시나리오를 바꿔도 예측이 덜 출렁이는 안정적인 시뮬레이터**를 만들기 위해 선택.

---

### 한 줄 정리
> **스냅샷 + FE/튜닝만으로는 F1 0.41, AUC 0.53에서 막혀 있었지만,  
> “시계열 + 고객접점” 신호를 추가하면서 F1 0.62+, AUC 0.79+까지 끌어올렸고,  
> 이후에는 풀 모델(HGB/XGB)과 6피처 LGBM 단조제약 앙상블을 각각  
> 메인 서비스 모델과 관리자용 시뮬레이터로 분리해 완성도를 높였다.**

### 최종 F1 / AUC 수치 요약
- **F1**: **0.33 → 0.41 → 0.62+ → 0.63+**
- **AUC**: **0.49 → 0.53 → 0.79+ → 0.81+**
  - (초기 원본 스냅샷 → FE 추가 → 시계열·고객접점 추가 RF → 최종 풀 모델(HGB))

