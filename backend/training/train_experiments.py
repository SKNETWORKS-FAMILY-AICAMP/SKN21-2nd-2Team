"""
train_experiments.py
Auth: ì‹ ì§€ìš©
ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸(`backend/preprocessing.py`)ì„ í˜¸ì¶œí•´
ëª¨ë¸ í•™ìŠµ + í‰ê°€(F1, AUC, Best Threshold)ë¥¼ ìˆ˜í–‰í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸.

í˜„ì¬ ì „ì²˜ë¦¬ ë¡œì§ì€ `notebooks/pipeline.ipynb`ì—ì„œ ì •ì˜ëœ
sklearn ColumnTransformer ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ì„ ê·¸ëŒ€ë¡œ ì˜®ê¸´
`preprocess_and_split` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì—­í•  ë¶„ë¦¬:
- ì „ì²˜ë¦¬ ìˆ˜ì •        â†’ `backend/preprocessing.py`
- ëª¨ë¸ ì¢…ë¥˜/íŒŒë¼ë¯¸í„° â†’ `backend/models.py`ì˜ `get_model()`
- ë°ì´í„° ê²½ë¡œ/seed/ë¹„ìœ¨ â†’ ì•„ë˜ CONFIG ìƒìˆ˜ë§Œ ìˆ˜ì •
"""

import json
import os
from datetime import datetime

import numpy as np
from sklearn.metrics import (
    f1_score,
    roc_auc_score,
    average_precision_score,
    precision_score,
    recall_score,
    confusion_matrix,
)

from backend.config import (
    DATA_PATH,
    TEST_SIZE,
    RANDOM_STATE,
    DEFAULT_MODEL_NAME,
    THRESH_START,
    THRESH_END,
    THRESH_STEP,
    METRICS_PATH,
)
from backend.models import get_model
from backend.preprocessing_pipeline import preprocess_and_split  # ê°™ì€ backend ë””ë ‰í„°ë¦¬ ê¸°ì¤€ import


# =========================================================
# ê³µí†µ ì„¤ì • (íŒ€ì›ì€ ë˜ë„ë¡ config.pyë§Œ ìˆ˜ì •)
# =========================================================
MODEL_NAME = DEFAULT_MODEL_NAME  # "rf", "logit", "hgb" ë“± backend/models.pyì—ì„œ ì§€ì›í•˜ëŠ” ì´ë¦„

# ì„ íƒ: í•˜ì´í¼íŒŒë¼ë¯¸í„° override (ê¸°ë³¸ì€ ë¹ˆ dict, í•„ìš”í•  ë•Œë§Œ ìˆ˜ì •)
MODEL_PARAMS = {
    # ì˜ˆì‹œ) RandomForest/ExtraTrees íŠœë‹
    # "n_estimators": 400,
    # "max_depth": 8,
    # "min_samples_leaf": 5,
    #
    # ì˜ˆì‹œ) XGBoost / LightGBM íŠœë‹
    # "learning_rate": 0.05,
    # "n_estimators": 400,
    # "max_depth": 6,
    # "n_estimators": 600,
    # "learning_rate": 0.03,
    # "max_depth": 3,
    # "subsample": 0.8,
    # "colsample_bytree": 0.8,
    # "scale_pos_weight": 3.0,

}


def evaluate_with_best_threshold(
    y_true,
    y_proba,
    thresholds: np.ndarray | None = None,
):
    """
    ì—¬ëŸ¬ thresholdë¥¼ ìŠ¤ìº”í•˜ì—¬ F1ì´ ìµœëŒ€ê°€ ë˜ëŠ” ì§€ì ì„ ì°¾ê³ ,
    ê·¸ë•Œì˜ F1ê³¼ ì „ì²´ AUCë¥¼ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if thresholds is None:
        thresholds = np.arange(THRESH_START, THRESH_END, THRESH_STEP)

    best_f1 = 0.0
    best_th = float(thresholds[0])
    best_precision = 0.0
    best_recall = 0.0

    for th in thresholds:
        y_pred = (y_proba >= th).astype(int)
        f1 = f1_score(y_true, y_pred)
        if f1 > best_f1:
            best_f1 = f1
            best_th = float(th)
            best_precision = precision_score(y_true, y_pred, zero_division=0)
            best_recall = recall_score(y_true, y_pred, zero_division=0)

    auc = roc_auc_score(y_true, y_proba)
    pr_auc = average_precision_score(y_true, y_proba)
    return best_f1, auc, pr_auc, best_th, best_precision, best_recall


def main():
    # 1) ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    #    - ë°ì´í„° ê²½ë¡œ/ë¹„ìœ¨/seedëŠ” ìƒë‹¨ CONFIGë¥¼ í†µí•´ ì œì–´
    #    - notebooks/pipeline.ipynbì™€ ë™ì¼í•œ sklearn ColumnTransformer íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
    X_train, X_test, y_train, y_test, _ = preprocess_and_split(
        path=DATA_PATH,
        test_size=TEST_SIZE,
        random_state=RANDOM_STATE,
    )

    # 2) ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
    #    - MODEL_NAMEì„ "rf", "xgb" ë“±ìœ¼ë¡œ ë°”ê¿”ê°€ë©° ì‹¤í—˜í•˜ê³ 
    #    - MODEL_PARAMSì— ì›í•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë§Œ ì„ íƒì ìœ¼ë¡œ ë„£ì–´ override ê°€ëŠ¥
    model = get_model(name=MODEL_NAME, random_state=RANDOM_STATE, **MODEL_PARAMS)
    model.fit(X_train, y_train)

    # 3) ì˜ˆì¸¡ ë° í‰ê°€
    y_proba = model.predict_proba(X_test)[:, 1]
    best_f1, auc, pr_auc, best_th, best_precision, best_recall = evaluate_with_best_threshold(
        y_test, y_proba
    )

    # Best threshold ê¸°ì¤€ ì˜ˆì¸¡ ê²°ê³¼ ë° í˜¼ë™ í–‰ë ¬
    y_pred_best = (y_proba >= best_th).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_best).ravel()

    print("ğŸ“Š Evaluation with sklearn preprocessing pipeline")
    print(f"- Model         : {MODEL_NAME}")
    print(f"- F1 Score      : {best_f1:.4f}")
    print(f"- AUC           : {auc:.4f}")
    print(f"- PR-AUC        : {pr_auc:.4f}")
    print(f"- Best Threshold: {best_th:.2f}")
    print(f"- Precision     : {best_precision:.4f}")
    print(f"- Recall        : {best_recall:.4f}")
    print(f"- Confusion     : TN={tn}, FP={fp}, FN={fn}, TP={tp}")
    print(f"- n_train       : {len(y_train)}")
    print(f"- n_test        : {len(y_test)}")

    # 4) ë©”íŠ¸ë¦­ ìë™ ì €ì¥
    save_metrics(
        model_name=MODEL_NAME,
        best_f1=best_f1,
        auc=auc,
        pr_auc=pr_auc,
        best_th=best_th,
        precision=best_precision,
        recall=best_recall,
        tn=tn,
        fp=fp,
        fn=fn,
        tp=tp,
        n_train=len(y_train),
        n_test=len(y_test),
    )


def save_metrics(
    model_name: str,
    best_f1: float,
    auc: float,
    pr_auc: float,
    best_th: float,
    precision: float,
    recall: float,
    tn: int,
    fp: int,
    fn: int,
    tp: int,
    n_train: int,
    n_test: int,
) -> None:
    """
    ì‹¤í—˜ ê²°ê³¼ ë©”íŠ¸ë¦­ì„ JSON íŒŒì¼ë¡œ ëˆ„ì  ì €ì¥í•©ë‹ˆë‹¤.
    - ì €ì¥ ìœ„ì¹˜: config.METRICS_PATH (ê¸°ë³¸: models/metrics.json)
    - í˜•ì‹: ì‹¤í–‰ë§ˆë‹¤ í•˜ë‚˜ì˜ dictë¥¼ ë¦¬ìŠ¤íŠ¸ì— append
    """
    # json.dump ì‹œ numpy íƒ€ì…(np.int64, np.float32 ë“±)ì„ ê·¸ëŒ€ë¡œ ë„£ìœ¼ë©´ ì—ëŸ¬ê°€ ë‚˜ë¯€ë¡œ
    # ì—¬ê¸°ì„œ ëª¨ë‘ Python ê¸°ë³¸ íƒ€ì…(int, float, str)ìœ¼ë¡œ ë³€í™˜í•´ ë‘”ë‹¤.
    run_info = {
        "model": str(model_name),
        "f1": float(best_f1),
        "auc": float(auc),
        "pr_auc": float(pr_auc),
        "best_threshold": float(best_th),
        "precision": float(precision),
        "recall": float(recall),
        "confusion_matrix": {
            "tn": int(tn),
            "fp": int(fp),
            "fn": int(fn),
            "tp": int(tp),
        },
        "n_train": int(n_train),
        "n_test": int(n_test),
        "data_path": str(DATA_PATH),
        "test_size": float(TEST_SIZE),
        "random_state": int(RANDOM_STATE),
        "threshold_range": {
            "start": float(THRESH_START),
            "end": float(THRESH_END),
            existing = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        existing = []

    if isinstance(existing, dict):
        # ì˜ˆì „ í˜•ì‹ì´ dict í•˜ë‚˜ì˜€ë‹¤ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        existing = [existing]

    existing.append(run_info)

    with open(metrics_path, "w", encoding="utf-8") as f:
        json.dump(existing, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ Metrics saved to {metrics_path}")


if __name__ == "__main__":
    main()


