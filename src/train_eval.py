"""
í•™ìŠµ/í‰ê°€ ë¡œì§ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë“ˆ.

âš  ìµœì¢… ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì´ sklearn Pipeline í˜•íƒœë¡œ ì œê³µë  ê²½ìš°,
   - (ì „ì²˜ë¦¬ê°€ í”¼ì²˜ ë ˆë²¨ì—ì„œ ì´ë¤„ì§„ë‹¤ë©´) train_test_split ì´í›„,
   - (ì›ë³¸ df ì „ì²´ë¥¼ ë‹¤ë£¨ëŠ” ê²½ìš°ë¼ë©´) data_loader ìª½ì—ì„œ
   í•´ë‹¹ íŒŒì´í”„ë¼ì¸ì„ ì ìš©í•˜ë©´ ëœë‹¤.
"""

from typing import Dict, Tuple

import numpy as np
from sklearn.metrics import f1_score, roc_auc_score
from sklearn.model_selection import train_test_split

from .config import RANDOM_STATE, TEST_SIZE


def train_and_evaluate(
    model,
    X,
    y,
    thresholds: np.ndarray | None = None,
) -> Tuple[Dict, object]:
    """
    ë‹¨ì¼ ëª¨ë¸ì— ëŒ€í•´ train/test split â†’ í•™ìŠµ â†’ threshold íŠœë‹ â†’ F1/AUC ê³„ì‚°ê¹Œì§€ ìˆ˜í–‰.

    - thresholdsê°€ Noneì´ë©´ [0.05, 0.35) êµ¬ê°„ì„ 0.01 ê°„ê²©ìœ¼ë¡œ ìŠ¤ìº”í•´
      F1ì´ ìµœëŒ€ê°€ ë˜ëŠ” best thresholdë¥¼ ì°¾ëŠ”ë‹¤.

    ë°˜í™˜:
        metrics (dict): F1, AUC, best_threshold ë“±
        trained_model: í•™ìŠµì´ ëë‚œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
    """
    if thresholds is None:
        thresholds = np.arange(0.05, 0.35, 0.01)
    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=TEST_SIZE,
        random_state=RANDOM_STATE,
        stratify=y,
    )

    # ------------------------------------------------------------------
    # ðŸ”Œ [ìµœì¢… ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í›… (HOOK)]
    #
    # ë§Œì•½ ë‹¤ë¥¸ íŒ€ì›ì´ sklearn.Pipeline í˜•íƒœì˜ ì „ì²˜ë¦¬/ëª¨ë¸ íŒŒì´í”„ë¼ì¸ì„
    # ì œê³µí•œë‹¤ë©´, ì‚¬ìš© ë°©ì‹ì— ë”°ë¼ ë‘ ê°€ì§€ íŒ¨í„´ì´ ìžˆì„ ìˆ˜ ìžˆë‹¤.
    #
    # 1) "ì „ì²˜ë¦¬ + ëª¨ë¸"ì´ í•˜ë‚˜ì˜ Pipelineì¸ ê²½ìš°:
    #       - ì´ í•¨ìˆ˜ì—ì„œ model ëŒ€ì‹  pipelineì„ ë°›ìœ¼ë©´ ë¨.
    #       - ì•„ëž˜ fit / predict_proba í˜¸ì¶œì€ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥.
    #
    # 2) "ì „ì²˜ë¦¬ Pipeline"ê³¼ "ëª¨ë¸"ì´ ë¶„ë¦¬ë˜ì–´ ìžˆëŠ” ê²½ìš°:
    #       from pipelines import final_pipeline
    #       X_train = final_pipeline.fit_transform(X_train)
    #       X_test = final_pipeline.transform(X_test)
    #
    #   ìœ„ì™€ ê°™ì´ ì´ ìœ„ì¹˜ì—ì„œ X_train/X_testì— íŒŒì´í”„ë¼ì¸ì„ ì ìš©í•˜ë©´ ëœë‹¤.
    # ------------------------------------------------------------------

    # ëª¨ë¸ í•™ìŠµ
    model.fit(X_train, y_train)

    # ì˜ˆì¸¡ í™•ë¥ 
    if hasattr(model, "predict_proba"):
        y_proba = model.predict_proba(X_test)[:, 1]
    else:
        # ì¼ë¶€ ëª¨ë¸ì€ decision_functionë§Œ ì œê³µí•  ìˆ˜ ìžˆìœ¼ë¯€ë¡œ,
        # ê·¸ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ fallback ë¡œì§
        if hasattr(model, "decision_function"):
            scores = model.decision_function(X_test)
            # scoresë¥¼ 0~1 í™•ë¥  ë¹„ìŠ·í•˜ê²Œ ìŠ¤ì¼€ì¼ë§ (ê°„ë‹¨í•œ min-max)
            scores_min = scores.min()
            scores_max = scores.max()
            if scores_max > scores_min:
                y_proba = (scores - scores_min) / (scores_max - scores_min)
            else:
                y_proba = np.zeros_like(scores, dtype=float)
        else:
            raise ValueError("ëª¨ë¸ì´ predict_probaë‚˜ decision_functionì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # AUC ê³„ì‚°
    auc = roc_auc_score(y_test, y_proba)

    # ìµœì  threshold íƒìƒ‰
    best_f1 = 0.0
    best_th = float(thresholds[0])

    for th in thresholds:
        y_pred_th = (y_proba >= th).astype(int)
        f1_th = f1_score(y_test, y_pred_th)
        if f1_th > best_f1:
            best_f1 = f1_th
            best_th = float(th)

    metrics = {
        "F1": best_f1,
        "AUC": auc,
        "best_threshold": best_th,
        "n_thresholds": len(thresholds),
        "n_samples": len(y),
    }

    return metrics, model


__all__ = ["train_and_evaluate"]


