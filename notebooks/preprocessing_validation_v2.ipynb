{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e967df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: c:\\2nd_project\\SKN21-2nd-2Team\\data\\enhanced_data.csv\n",
      "\n",
      "Shape: (8000, 26)\n",
      "\n",
      "Dtypes:\n",
      "user_id                       int64\n",
      "gender                       object\n",
      "age                           int64\n",
      "country                      object\n",
      "subscription_type            object\n",
      "listening_time              float64\n",
      "songs_played_per_day        float64\n",
      "skip_rate                   float64\n",
      "device_type                  object\n",
      "ads_listened_per_week         int64\n",
      "offline_listening             int64\n",
      "is_churned                    int64\n",
      "engagement_score            float64\n",
      "songs_per_minute            float64\n",
      "skip_intensity              float64\n",
      "skip_rate_cap               float64\n",
      "ads_pressure                float64\n",
      "listening_time_trend_7d     float64\n",
      "login_frequency_30d           int64\n",
      "days_since_last_login         int64\n",
      "skip_rate_increase_7d       float64\n",
      "freq_of_use_trend_14d       float64\n",
      "customer_support_contact      int64\n",
      "payment_failure_count         int64\n",
      "promotional_email_click       int64\n",
      "app_crash_count_30d           int64\n",
      "dtype: object\n",
      "\n",
      "Missing values (count):\n",
      "engagement_score            473\n",
      "songs_played_per_day        240\n",
      "skip_intensity              240\n",
      "listening_time              240\n",
      "gender                        0\n",
      "user_id                       0\n",
      "subscription_type             0\n",
      "skip_rate                     0\n",
      "country                       0\n",
      "age                           0\n",
      "ads_listened_per_week         0\n",
      "device_type                   0\n",
      "is_churned                    0\n",
      "offline_listening             0\n",
      "songs_per_minute              0\n",
      "skip_rate_cap                 0\n",
      "ads_pressure                  0\n",
      "listening_time_trend_7d       0\n",
      "login_frequency_30d           0\n",
      "days_since_last_login         0\n",
      "skip_rate_increase_7d         0\n",
      "freq_of_use_trend_14d         0\n",
      "customer_support_contact      0\n",
      "payment_failure_count         0\n",
      "promotional_email_click       0\n",
      "app_crash_count_30d           0\n",
      "dtype: int64\n",
      "\n",
      "Missing values (ratio %):\n",
      "engagement_score            5.912\n",
      "songs_played_per_day        3.000\n",
      "skip_intensity              3.000\n",
      "listening_time              3.000\n",
      "gender                      0.000\n",
      "user_id                     0.000\n",
      "subscription_type           0.000\n",
      "skip_rate                   0.000\n",
      "country                     0.000\n",
      "age                         0.000\n",
      "ads_listened_per_week       0.000\n",
      "device_type                 0.000\n",
      "is_churned                  0.000\n",
      "offline_listening           0.000\n",
      "songs_per_minute            0.000\n",
      "skip_rate_cap               0.000\n",
      "ads_pressure                0.000\n",
      "listening_time_trend_7d     0.000\n",
      "login_frequency_30d         0.000\n",
      "days_since_last_login       0.000\n",
      "skip_rate_increase_7d       0.000\n",
      "freq_of_use_trend_14d       0.000\n",
      "customer_support_contact    0.000\n",
      "payment_failure_count       0.000\n",
      "promotional_email_click     0.000\n",
      "app_crash_count_30d         0.000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ë°ì´í„° ê²½ë¡œ ì„¤ì •\n",
    "PROJECT_ROOT = Path.cwd().parent  # í˜„ì¬ ìœ„ì¹˜ê°€ notebooks/ ë¼ê³  ê°€ì •\n",
    "DATA_PATH = PROJECT_ROOT / \"data\" / \"enhanced_data.csv\"\n",
    "\n",
    "print(\"Data path:\", DATA_PATH)\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"\\nShape:\", df.shape)\n",
    "print(\"\\nDtypes:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nMissing values (count):\")\n",
    "print(df.isna().sum().sort_values(ascending=False))\n",
    "\n",
    "print(\"\\nMissing values (ratio %):\")\n",
    "print((df.isna().mean() * 100).sort_values(ascending=False).round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b68d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After imputation - missing values (count):\n",
      "engagement_score           473\n",
      "songs_played_per_day       240\n",
      "skip_intensity             240\n",
      "listening_time             240\n",
      "gender                       0\n",
      "user_id                      0\n",
      "subscription_type            0\n",
      "skip_rate                    0\n",
      "country                      0\n",
      "age                          0\n",
      "ads_listened_per_week        0\n",
      "device_type                  0\n",
      "is_churned                   0\n",
      "offline_listening            0\n",
      "songs_per_minute             0\n",
      "skip_rate_cap                0\n",
      "ads_pressure                 0\n",
      "listening_time_trend_7d      0\n",
      "login_frequency_30d          0\n",
      "days_since_last_login        0\n",
      "dtype: int64\n",
      "\n",
      "Saved imputed data to: c:\\2nd_project\\SKN21-2nd-2Team\\data\\enhanced_data_imputed.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5b2d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ê²°ì¸¡ì¹˜ ìš”ì•½]\n",
      "- ì›ë³¸ ë°ì´í„°(df) ì „ì²´ ê²°ì¸¡ ê°œìˆ˜: 1193\n",
      "- ê²°ì¸¡ ì²˜ë¦¬ í›„(df_imputed) ì „ì²´ ê²°ì¸¡ ê°œìˆ˜: 1193\n",
      "\n",
      "[ì´ìƒì¹˜ ìš”ì•½ - IQR ê¸°ì¤€]\n",
      "- ì´ìƒì¹˜(í•œ ì»¬ëŸ¼ì´ë¼ë„ IQR ë²”ìœ„ ë°–ì¸ í–‰) í¬í•¨ í–‰ ìˆ˜: 2833\n",
      "\n",
      "- ì»¬ëŸ¼ë³„ ì´ìƒì¹˜ ê°œìˆ˜ Top 10 (IQR ê¸°ì¤€):\n",
      "  ads_listened_per_week: 1683\n",
      "  songs_per_minute: 826\n",
      "  payment_failure_count: 389\n",
      "  skip_intensity: 170\n",
      "  skip_rate: 160\n",
      "  skip_rate_cap: 160\n",
      "  engagement_score: 81\n",
      "  age: 80\n",
      "  user_id: 0\n",
      "  listening_time: 0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "166f433c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW_PATH: c:\\2nd_project\\SKN21-2nd-2Team\\data\\raw_data.csv\n",
      "SYN_PATH: c:\\2nd_project\\SKN21-2nd-2Team\\data\\synthetic_features.csv\n",
      "ENH_PATH: c:\\2nd_project\\SKN21-2nd-2Team\\data\\enhanced_data_clean.csv\n",
      "\n",
      "[ê¸°ë³¸ ì •ë³´]\n",
      "- raw_data: shape=(8000, 12), ì „ì²´ ê²°ì¸¡ ê°œìˆ˜=480\n",
      "- synthetic_features: shape=(8000, 10), ì „ì²´ ê²°ì¸¡ ê°œìˆ˜=0\n",
      "- enhanced_data: shape=(8000, 26), ì „ì²´ ê²°ì¸¡ ê°œìˆ˜=0\n",
      "\n",
      "[raw_data - ì´ìƒì¹˜ ìš”ì•½ (IQR ê¸°ì¤€)]\n",
      "- ì´ìƒì¹˜(í•œ ì»¬ëŸ¼ì´ë¼ë„ ë²”ìœ„ ë°–ì¸ í–‰ ìˆ˜): 1878\n",
      "- ì»¬ëŸ¼ë³„ ì´ìƒì¹˜ ê°œìˆ˜ Top 5:\n",
      "  ads_listened_per_week: 1683\n",
      "  skip_rate: 160\n",
      "  age: 80\n",
      "  user_id: 0\n",
      "  listening_time: 0\n",
      "\n",
      "[synthetic_features - ì´ìƒì¹˜ ìš”ì•½ (IQR ê¸°ì¤€)]\n",
      "- ì´ìƒì¹˜(í•œ ì»¬ëŸ¼ì´ë¼ë„ ë²”ìœ„ ë°–ì¸ í–‰ ìˆ˜): 389\n",
      "- ì»¬ëŸ¼ë³„ ì´ìƒì¹˜ ê°œìˆ˜ Top 5:\n",
      "  payment_failure_count: 389\n",
      "  user_id: 0\n",
      "  listening_time_trend_7d: 0\n",
      "  login_frequency_30d: 0\n",
      "  days_since_last_login: 0\n",
      "\n",
      "[enhanced_data - ì´ìƒì¹˜ ìš”ì•½ (IQR ê¸°ì¤€)]\n",
      "- ì´ìƒì¹˜(í•œ ì»¬ëŸ¼ì´ë¼ë„ ë²”ìœ„ ë°–ì¸ í–‰ ìˆ˜): 175\n",
      "- ì»¬ëŸ¼ë³„ ì´ìƒì¹˜ ê°œìˆ˜ Top 5:\n",
      "  skip_intensity: 175\n",
      "  user_id: 0\n",
      "  age: 0\n",
      "  listening_time: 0\n",
      "  songs_played_per_day: 0\n"
     ]
    }
   ],
   "source": [
    "# raw_data, synthetic_features, enhanced_data ê°„ ê²°ì¸¡ì¹˜/ì´ìƒì¹˜ ë¹„êµ\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "RAW_PATH = PROJECT_ROOT / \"data\" / \"raw_data.csv\"\n",
    "SYN_PATH = PROJECT_ROOT / \"data\" / \"synthetic_features.csv\"\n",
    "ENH_PATH = PROJECT_ROOT / \"data\" / \"enhanced_data_clean.csv\"\n",
    "\n",
    "print(\"RAW_PATH:\", RAW_PATH)\n",
    "print(\"SYN_PATH:\", SYN_PATH)\n",
    "print(\"ENH_PATH:\", ENH_PATH)\n",
    "\n",
    "raw_df = pd.read_csv(RAW_PATH)\n",
    "syn_df = pd.read_csv(SYN_PATH)\n",
    "enh_df = pd.read_csv(ENH_PATH)\n",
    "\n",
    "print(\"\\n[ê¸°ë³¸ ì •ë³´]\")\n",
    "for name, d in [(\"raw_data\", raw_df), (\"synthetic_features\", syn_df), (\"enhanced_data\", enh_df)]:\n",
    "    print(f\"- {name}: shape={d.shape}, ì „ì²´ ê²°ì¸¡ ê°œìˆ˜={int(d.isna().sum().sum())}\")\n",
    "\n",
    "\n",
    "def summarize_outliers(name, df):\n",
    "    \"\"\"IQR ê¸°ì¤€ ì´ìƒì¹˜ í–‰ ìˆ˜ ë° ì»¬ëŸ¼ë³„ ì´ìƒì¹˜ ê°œìˆ˜ ìš”ì•½\"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "    outlier_mask_global = np.zeros(len(df), dtype=bool)\n",
    "    outlier_counts_per_col = {}\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        series = df[col].dropna()\n",
    "        if series.empty:\n",
    "            continue\n",
    "        q1 = series.quantile(0.25)\n",
    "        q3 = series.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        if iqr == 0:\n",
    "            continue\n",
    "        lower = q1 - 1.5 * iqr\n",
    "        upper = q3 + 1.5 * iqr\n",
    "        mask = (df[col] < lower) | (df[col] > upper)\n",
    "        count = int(mask.sum())\n",
    "        outlier_counts_per_col[col] = count\n",
    "        outlier_mask_global |= mask\n",
    "\n",
    "    total_outlier_rows = int(outlier_mask_global.sum())\n",
    "\n",
    "    print(f\"\\n[{name} - ì´ìƒì¹˜ ìš”ì•½ (IQR ê¸°ì¤€)]\")\n",
    "    print(f\"- ì´ìƒì¹˜(í•œ ì»¬ëŸ¼ì´ë¼ë„ ë²”ìœ„ ë°–ì¸ í–‰ ìˆ˜): {total_outlier_rows}\")\n",
    "\n",
    "    print(\"- ì»¬ëŸ¼ë³„ ì´ìƒì¹˜ ê°œìˆ˜ Top 5:\")\n",
    "    for col, cnt in sorted(outlier_counts_per_col.items(), key=lambda x: -x[1])[:5]:\n",
    "        print(f\"  {col}: {cnt}\")\n",
    "\n",
    "\n",
    "summarize_outliers(\"raw_data\", raw_df)\n",
    "summarize_outliers(\"synthetic_features\", syn_df)\n",
    "summarize_outliers(\"enhanced_data\", enh_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c75601cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸ enhanced_data shape: (8000, 26)\n",
      "ì›ë³¸ ì „ì²´ ê²°ì¸¡ ê°œìˆ˜: 1193\n",
      "listening_time: ê²°ì¸¡ì¹˜ë¥¼ median(154.000)ìœ¼ë¡œ ëŒ€ì²´\n",
      "songs_played_per_day: ê²°ì¸¡ì¹˜ë¥¼ median(50.000)ìœ¼ë¡œ ëŒ€ì²´\n",
      "engagement_score, skip_intensity íŒŒìƒ ì»¬ëŸ¼ ì¬ê³„ì‚° ì™„ë£Œ\n",
      "\n",
      "ê²°ì¸¡ ì²˜ë¦¬ í›„ ì „ì²´ ê²°ì¸¡ ê°œìˆ˜: 0\n",
      "\n",
      "[IQR ê¸°ë°˜ ì´ìƒì¹˜ winsorizing ì ìš© ì»¬ëŸ¼ ìˆ˜] 20\n",
      "\n",
      "[IQR ê¸°ì¤€ ì´ìƒì¹˜ ìš”ì•½ - ì²˜ë¦¬ í›„]\n",
      "- ì´ìƒì¹˜(í•œ ì»¬ëŸ¼ì´ë¼ë„ ë²”ìœ„ ë°–ì¸ í–‰ ìˆ˜): 0\n",
      "- ì»¬ëŸ¼ë³„ ì´ìƒì¹˜ ê°œìˆ˜ Top 5:\n",
      "  age: 0\n",
      "  listening_time: 0\n",
      "  songs_played_per_day: 0\n",
      "  skip_rate: 0\n",
      "  ads_listened_per_week: 0\n",
      "\n",
      "clean ë°ì´í„° ì €ì¥ ì™„ë£Œ: c:\\2nd_project\\SKN21-2nd-2Team\\data\\enhanced_data_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# enhanced_dataì—ì„œ ê²°ì¸¡ì¹˜ + ì´ìƒì¹˜ ì²˜ë¦¬ í›„ clean CSV ìƒì„±\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "ENH_PATH = PROJECT_ROOT / \"data\" / \"enhanced_data.csv\"\n",
    "\n",
    "enh = pd.read_csv(ENH_PATH)\n",
    "print(\"ì›ë³¸ enhanced_data shape:\", enh.shape)\n",
    "print(\"ì›ë³¸ ì „ì²´ ê²°ì¸¡ ê°œìˆ˜:\", int(enh.isna().sum().sum()))\n",
    "\n",
    "# 1) ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "# ì›ë³¸ ìˆ˜ì¹˜í˜•ì—ì„œ ê²°ì¸¡ì´ ìˆëŠ” ì»¬ëŸ¼: listening_time, songs_played_per_day\n",
    "missing_base_cols = [\"listening_time\", \"songs_played_per_day\"]\n",
    "\n",
    "for col in missing_base_cols:\n",
    "    if col in enh.columns:\n",
    "        median_val = enh[col].median()\n",
    "        enh[col] = enh[col].fillna(median_val)\n",
    "        print(f\"{col}: ê²°ì¸¡ì¹˜ë¥¼ median({median_val:.3f})ìœ¼ë¡œ ëŒ€ì²´\")\n",
    "\n",
    "# íŒŒìƒ ì»¬ëŸ¼ì€ ë‹¤ì‹œ ê³„ì‚°í•´ì„œ NaN ì œê±° (ì¼ê´€ì„± ìœ ì§€)\n",
    "if set([\"listening_time\", \"songs_played_per_day\"]).issubset(enh.columns):\n",
    "    enh[\"engagement_score\"] = enh[\"listening_time\"] * enh[\"songs_played_per_day\"]\n",
    "    enh[\"skip_intensity\"] = enh[\"skip_rate\"] * enh[\"songs_played_per_day\"]\n",
    "    print(\"engagement_score, skip_intensity íŒŒìƒ ì»¬ëŸ¼ ì¬ê³„ì‚° ì™„ë£Œ\")\n",
    "\n",
    "print(\"\\nê²°ì¸¡ ì²˜ë¦¬ í›„ ì „ì²´ ê²°ì¸¡ ê°œìˆ˜:\", int(enh.isna().sum().sum()))\n",
    "\n",
    "# 2) ì´ìƒì¹˜ ì²˜ë¦¬ (IQR ê¸°ë°˜ winsorizing)\n",
    "# user_id, is_churnedëŠ” ëŒ€ìƒì—ì„œ ì œì™¸\n",
    "numeric_cols = enh.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "numeric_cols = [c for c in numeric_cols if c not in [\"user_id\", \"is_churned\"]]\n",
    "\n",
    "print(\"\\n[IQR ê¸°ë°˜ ì´ìƒì¹˜ winsorizing ì ìš© ì»¬ëŸ¼ ìˆ˜]\", len(numeric_cols))\n",
    "\n",
    "for col in numeric_cols:\n",
    "    series = enh[col].dropna()\n",
    "    if series.empty:\n",
    "        continue\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    if iqr == 0:\n",
    "        continue\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    enh[col] = enh[col].clip(lower, upper)\n",
    "\n",
    "# ì´ìƒì¹˜ ì²˜ë¦¬ í›„ IQR ê¸°ì¤€ ì´ìƒì¹˜ í–‰ ìˆ˜ ì¬ê³„ì‚°\n",
    "from collections import defaultdict\n",
    "\n",
    "outlier_mask_global = np.zeros(len(enh), dtype=bool)\n",
    "outlier_counts_per_col = defaultdict(int)\n",
    "\n",
    "for col in numeric_cols:\n",
    "    series = enh[col].dropna()\n",
    "    if series.empty:\n",
    "        continue\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    if iqr == 0:\n",
    "        continue\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    mask = (enh[col] < lower) | (enh[col] > upper)\n",
    "    cnt = int(mask.sum())\n",
    "    outlier_counts_per_col[col] = cnt\n",
    "    outlier_mask_global |= mask\n",
    "\n",
    "total_outlier_rows = int(outlier_mask_global.sum())\n",
    "print(\"\\n[IQR ê¸°ì¤€ ì´ìƒì¹˜ ìš”ì•½ - ì²˜ë¦¬ í›„]\")\n",
    "print(\"- ì´ìƒì¹˜(í•œ ì»¬ëŸ¼ì´ë¼ë„ ë²”ìœ„ ë°–ì¸ í–‰ ìˆ˜):\", total_outlier_rows)\n",
    "print(\"- ì»¬ëŸ¼ë³„ ì´ìƒì¹˜ ê°œìˆ˜ Top 5:\")\n",
    "for col, cnt in sorted(outlier_counts_per_col.items(), key=lambda x: -x[1])[:5]:\n",
    "    print(f\"  {col}: {cnt}\")\n",
    "\n",
    "# 3) clean CSV ì €ì¥\n",
    "CLEAN_PATH = PROJECT_ROOT / \"data\" / \"enhanced_data_clean.csv\"\n",
    "enh.to_csv(CLEAN_PATH, index=False)\n",
    "print(f\"\\nclean ë°ì´í„° ì €ì¥ ì™„ë£Œ: {CLEAN_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd9b2e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean ë°ì´í„° shape: (8000, 26)\n",
      "ì‚¬ìš© í”¼ì²˜ ìˆ˜: 20\n",
      "\n",
      "[RandomForest ì„±ëŠ¥ - enhanced_data_clean ê¸°ì¤€]\n",
      "F1: 0.5574\n",
      "AUC: 0.7933\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.94      0.88      1186\n",
      "           1       0.73      0.45      0.56       414\n",
      "\n",
      "    accuracy                           0.81      1600\n",
      "   macro avg       0.78      0.70      0.72      1600\n",
      "weighted avg       0.80      0.81      0.80      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# enhanced_data_clean.csv ê¸°ì¤€ RandomForest ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "CLEAN_PATH = PROJECT_ROOT / \"data\" / \"enhanced_data_clean.csv\"\n",
    "\n",
    "df_rf = pd.read_csv(CLEAN_PATH)\n",
    "print(\"clean ë°ì´í„° shape:\", df_rf.shape)\n",
    "\n",
    "# íƒ€ê¹ƒê³¼ í”¼ì²˜ ë¶„ë¦¬\n",
    "y = df_rf[\"is_churned\"]\n",
    "\n",
    "# ìˆ˜ì¹˜í˜• í”¼ì²˜ë§Œ ì‚¬ìš© (ë²”ì£¼í˜• ì¸ì½”ë”© ì—†ì´ ë¹ ë¥´ê²Œ ì„±ëŠ¥ í™•ì¸)\n",
    "feature_cols = df_rf.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "feature_cols = [c for c in feature_cols if c not in [\"user_id\", \"is_churned\"]]\n",
    "X = df_rf[feature_cols]\n",
    "\n",
    "print(\"ì‚¬ìš© í”¼ì²˜ ìˆ˜:\", len(feature_cols))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "if hasattr(rf, \"predict_proba\"):\n",
    "    y_proba = rf.predict_proba(X_test)[:, 1]\n",
    "else:\n",
    "    # predict_probaê°€ ì—†ì„ ê²½ìš° decision_function ì‚¬ìš© (ì˜ˆë¹„ìš©)\n",
    "    from sklearn.metrics import roc_curve\n",
    "    scores = rf.decision_function(X_test)\n",
    "    # ì ìˆ˜ë¥¼ 0~1ë¡œ ìŠ¤ì¼€ì¼ë§\n",
    "    y_proba = (scores - scores.min()) / (scores.max() - scores.min())\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(\"\\n[RandomForest ì„±ëŠ¥ - enhanced_data_clean ê¸°ì¤€]\")\n",
    "print(\"F1:\", round(f1, 4))\n",
    "print(\"AUC:\", round(auc, 4))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2906d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²´ ìˆ˜ì¹˜í˜• í”¼ì²˜ ìˆ˜ (FE í¬í•¨): 20\n",
      "FE ì œì™¸ í”¼ì²˜ ìˆ˜: 15\n",
      "\n",
      "[FE í¬í•¨ (ëª¨ë“  ìˆ˜ì¹˜í˜•)] F1=0.5574, AUC=0.7933\n",
      "\n",
      "[FE 5ê°œ ì œì™¸] F1=0.5810, AUC=0.7946\n"
     ]
    }
   ],
   "source": [
    "# FE í¬í•¨ vs ì œì™¸ RF ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "CLEAN_PATH = PROJECT_ROOT / \"data\" / \"enhanced_data_clean.csv\"\n",
    "\n",
    "df_cmp = pd.read_csv(CLEAN_PATH)\n",
    "\n",
    "# ê³µí†µ: íƒ€ê¹ƒ\n",
    "y = df_cmp[\"is_churned\"]\n",
    "\n",
    "# FE 5ê°œ (ë¬¸ì„œ ê¸°ì¤€)\n",
    "fe_cols = [\n",
    "    \"engagement_score\",\n",
    "    \"songs_per_minute\",\n",
    "    \"skip_intensity\",\n",
    "    \"skip_rate_cap\",\n",
    "    \"ads_pressure\",\n",
    "]\n",
    "\n",
    "# ìˆ˜ì¹˜í˜• ì „ì²´ (user_id, target ì œì™¸)\n",
    "all_num_cols = df_cmp.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "all_num_cols = [c for c in all_num_cols if c not in [\"user_id\", \"is_churned\"]]\n",
    "\n",
    "# 1) FE í¬í•¨ (í˜„ì¬ ìš°ë¦¬ê°€ ì“°ëŠ” êµ¬ì„±ê³¼ ë¹„ìŠ·)\n",
    "X_all = df_cmp[all_num_cols]\n",
    "\n",
    "# 2) FE 5ê°œ ì œì™¸ (ì›ë³¸ ìˆ˜ì¹˜í˜• + ì‹œê³„ì—´/ê³ ê°ì ‘ì ë§Œ)\n",
    "cols_wo_fe = [c for c in all_num_cols if c not in fe_cols]\n",
    "X_wo_fe = df_cmp[cols_wo_fe]\n",
    "\n",
    "print(\"ì „ì²´ ìˆ˜ì¹˜í˜• í”¼ì²˜ ìˆ˜ (FE í¬í•¨):\", len(all_num_cols))\n",
    "print(\"FE ì œì™¸ í”¼ì²˜ ìˆ˜:\", len(cols_wo_fe))\n",
    "\n",
    "\n",
    "def eval_rf(X, y, name):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        random_state=42,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    y_proba = rf.predict_proba(X_test)[:, 1]\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f\"\\n[{name}] F1={f1:.4f}, AUC={auc:.4f}\")\n",
    "    return f1, auc\n",
    "\n",
    "f1_all, auc_all = eval_rf(X_all, y, \"FE í¬í•¨ (ëª¨ë“  ìˆ˜ì¹˜í˜•)\")\n",
    "f1_wo, auc_wo = eval_rf(X_wo_fe, y, \"FE 5ê°œ ì œì™¸\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a623b66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸ enhanced_data shape: (8000, 26)\n",
      "ì›ë³¸ ì „ì²´ ê²°ì¸¡ ê°œìˆ˜: 1193\n",
      "ì›ë³¸ ìˆ˜ì¹˜í˜• í”¼ì²˜ ìˆ˜ (FE í¬í•¨): 20\n",
      "ì›ë³¸ FE ì œì™¸ í”¼ì²˜ ìˆ˜: 15\n",
      "[ì›ë³¸] FE í¬í•¨ - ì‚¬ìš© í–‰ ìˆ˜: 7527 (ë“œë¡­ëœ í–‰: 473)\n",
      "  â†’ F1=0.6244, AUC=0.8207\n",
      "\n",
      "[ì›ë³¸] FE ì œì™¸ - ì‚¬ìš© í–‰ ìˆ˜: 7527 (ë“œë¡­ëœ í–‰: 473)\n",
      "  â†’ F1=0.6338, AUC=0.8280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# [ì¶”ê°€ ì‹¤í—˜] ì›ë³¸ enhanced_data (ê²°ì¸¡ ì²˜ë¦¬/ìœˆì €ë¼ì´ì§• ì—†ì´)ì—ì„œ FE í¬í•¨ vs ì œì™¸ ë¹„êµ\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "ENH_PATH = PROJECT_ROOT / \"data\" / \"enhanced_data.csv\"\n",
    "\n",
    "df_raw_enh = pd.read_csv(ENH_PATH)\n",
    "print(\"ì›ë³¸ enhanced_data shape:\", df_raw_enh.shape)\n",
    "print(\"ì›ë³¸ ì „ì²´ ê²°ì¸¡ ê°œìˆ˜:\", int(df_raw_enh.isna().sum().sum()))\n",
    "\n",
    "# íƒ€ê¹ƒ\n",
    "y_full = df_raw_enh[\"is_churned\"]\n",
    "\n",
    "# FE 5ê°œ\n",
    "fe_cols = [\n",
    "    \"engagement_score\",\n",
    "    \"songs_per_minute\",\n",
    "    \"skip_intensity\",\n",
    "    \"skip_rate_cap\",\n",
    "    \"ads_pressure\",\n",
    "]\n",
    "\n",
    "# ìˆ˜ì¹˜í˜• ì „ì²´ (user_id, target ì œì™¸)\n",
    "all_num_cols = df_raw_enh.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "all_num_cols = [c for c in all_num_cols if c not in [\"user_id\", \"is_churned\"]]\n",
    "\n",
    "# 1) FE í¬í•¨\n",
    "X_all_full = df_raw_enh[all_num_cols]\n",
    "\n",
    "# 2) FE ì œì™¸\n",
    "cols_wo_fe = [c for c in all_num_cols if c not in fe_cols]\n",
    "X_wo_fe_full = df_raw_enh[cols_wo_fe]\n",
    "\n",
    "print(\"ì›ë³¸ ìˆ˜ì¹˜í˜• í”¼ì²˜ ìˆ˜ (FE í¬í•¨):\", len(all_num_cols))\n",
    "print(\"ì›ë³¸ FE ì œì™¸ í”¼ì²˜ ìˆ˜:\", len(cols_wo_fe))\n",
    "\n",
    "\n",
    "def eval_rf_dropna(X_full, y_full, name):\n",
    "    # sklearnì€ NaNì„ ì§ì ‘ ì²˜ë¦¬ ëª»í•˜ë¯€ë¡œ, ì‚¬ìš© í”¼ì²˜ ê¸°ì¤€ìœ¼ë¡œ NaN í–‰ì€ ë“œë¡­\n",
    "    mask = X_full.notna().all(axis=1)\n",
    "    X = X_full.loc[mask].copy()\n",
    "    y = y_full.loc[mask].copy()\n",
    "    print(f\"{name} - ì‚¬ìš© í–‰ ìˆ˜: {X.shape[0]} (ë“œë¡­ëœ í–‰: {len(y_full) - len(y)})\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        random_state=42,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    y_proba = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f\"  â†’ F1={f1:.4f}, AUC={auc:.4f}\\n\")\n",
    "    return f1, auc\n",
    "\n",
    "f1_all_raw, auc_all_raw = eval_rf_dropna(X_all_full, y_full, \"[ì›ë³¸] FE í¬í•¨\")\n",
    "f1_wo_raw, auc_wo_raw = eval_rf_dropna(X_wo_fe_full, y_full, \"[ì›ë³¸] FE ì œì™¸\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5943f8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_enh shape: (8000, 26)  / ì „ì²´ ê²°ì¸¡: 1193\n",
      "clean_enh shape: (8000, 26)  / ì „ì²´ ê²°ì¸¡: 0\n",
      "ê³µí†µ ìˆ˜ì¹˜í˜• í”¼ì²˜ ìˆ˜ (FE ì œê±° í›„): 15\n",
      "ê³µí†µ í”¼ì²˜ ì˜ˆì‹œ: ['ads_listened_per_week', 'age', 'app_crash_count_30d', 'customer_support_contact', 'days_since_last_login', 'freq_of_use_trend_14d', 'listening_time', 'listening_time_trend_7d', 'login_frequency_30d', 'offline_listening']\n",
      "[ì›ë³¸] FE ì œê±° + ê²°ì¸¡ drop - ì‚¬ìš© í–‰ ìˆ˜: 7527 (ë“œë¡­ëœ í–‰: 473)\n",
      "  â†’ F1=0.6151, AUC=0.8287\n",
      "\n",
      "[clean] FE ì œê±° + ê²°ì¸¡/ì´ìƒì¹˜ ì²˜ë¦¬ - ì‚¬ìš© í–‰ ìˆ˜: 8000 (ë“œë¡­ëœ í–‰: 0)\n",
      "  â†’ F1=0.5664, AUC=0.7957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FE ì™„ì „ ì œê±° + ì‹œê³„ì—´/ê³ ê°ì ‘ì  ìœ ì§€ ìƒíƒœì—ì„œ\n",
    "# [ì›ë³¸ enhanced_data (ê²°ì¸¡ drop)] vs [enhanced_data_clean (ê²°ì¸¡/ì´ìƒì¹˜ ì²˜ë¦¬)] RF ì„±ëŠ¥ ë¹„êµ\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "ENH_PATH = PROJECT_ROOT / \"data\" / \"enhanced_data.csv\"\n",
    "CLEAN_PATH = PROJECT_ROOT / \"data\" / \"enhanced_data_clean.csv\"\n",
    "\n",
    "raw_enh = pd.read_csv(ENH_PATH)\n",
    "clean_enh = pd.read_csv(CLEAN_PATH)\n",
    "\n",
    "print(\"raw_enh shape:\", raw_enh.shape, \" / ì „ì²´ ê²°ì¸¡:\", int(raw_enh.isna().sum().sum()))\n",
    "print(\"clean_enh shape:\", clean_enh.shape, \" / ì „ì²´ ê²°ì¸¡:\", int(clean_enh.isna().sum().sum()))\n",
    "\n",
    "# íƒ€ê¹ƒ\n",
    "y_raw = raw_enh[\"is_churned\"]\n",
    "y_clean = clean_enh[\"is_churned\"]\n",
    "\n",
    "# FE 5ê°œëŠ” ì™„ì „íˆ ì œê±°\n",
    "fe_cols = [\n",
    "    \"engagement_score\",\n",
    "    \"songs_per_minute\",\n",
    "    \"skip_intensity\",\n",
    "    \"skip_rate_cap\",\n",
    "    \"ads_pressure\",\n",
    "]\n",
    "\n",
    "# ê³µí†µìœ¼ë¡œ ì“¸ ìˆ˜ì¹˜í˜• í”¼ì²˜ ëª©ë¡ (user_id, is_churned, FE ì œì™¸)\n",
    "num_cols_raw = raw_enh.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "num_cols_raw = [c for c in num_cols_raw if c not in [\"user_id\", \"is_churned\"] + fe_cols]\n",
    "\n",
    "num_cols_clean = clean_enh.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "num_cols_clean = [c for c in num_cols_clean if c not in [\"user_id\", \"is_churned\"] + fe_cols]\n",
    "\n",
    "# ì–‘ìª½ì—ì„œ ê³µí†µìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì‚¬ìš© (í˜¹ì‹œë¼ë„ íƒ€ì…/ì´ë¦„ ì°¨ì´ ë°©ì§€)\n",
    "common_cols = sorted(list(set(num_cols_raw) & set(num_cols_clean)))\n",
    "\n",
    "X_raw_full = raw_enh[common_cols]\n",
    "X_clean_full = clean_enh[common_cols]\n",
    "\n",
    "print(\"ê³µí†µ ìˆ˜ì¹˜í˜• í”¼ì²˜ ìˆ˜ (FE ì œê±° í›„):\", len(common_cols))\n",
    "print(\"ê³µí†µ í”¼ì²˜ ì˜ˆì‹œ:\", common_cols[:10])\n",
    "\n",
    "\n",
    "def eval_rf_with_optional_dropna(X_full, y_full, name, dropna=True):\n",
    "    # dropna=Trueì´ë©´ í•´ë‹¹ í”¼ì²˜ ê¸°ì¤€ NaN ìˆëŠ” í–‰ ì œê±° (ì›ë³¸ìš©)\n",
    "    if dropna:\n",
    "        mask = X_full.notna().all(axis=1)\n",
    "        X = X_full.loc[mask].copy()\n",
    "        y = y_full.loc[mask].copy()\n",
    "        dropped = len(y_full) - len(y)\n",
    "    else:\n",
    "        X = X_full.copy()\n",
    "        y = y_full.copy()\n",
    "        dropped = 0\n",
    "\n",
    "    print(f\"{name} - ì‚¬ìš© í–‰ ìˆ˜: {X.shape[0]} (ë“œë¡­ëœ í–‰: {dropped})\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        random_state=42,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    y_proba = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f\"  â†’ F1={f1:.4f}, AUC={auc:.4f}\\n\")\n",
    "    return f1, auc\n",
    "\n",
    "# 1) ì›ë³¸ enhanced_data: ê²°ì¸¡ drop ê¸°ë°˜\n",
    "f1_raw_fe_off, auc_raw_fe_off = eval_rf_with_optional_dropna(\n",
    "    X_raw_full, y_raw, \"[ì›ë³¸] FE ì œê±° + ê²°ì¸¡ drop\", dropna=True\n",
    ")\n",
    "\n",
    "# 2) clean ë°ì´í„°: ê²°ì¸¡/ì´ìƒì¹˜ ì²˜ë¦¬ í›„, drop ì—†ì´ ì‚¬ìš©\n",
    "f1_clean_fe_off, auc_clean_fe_off = eval_rf_with_optional_dropna(\n",
    "    X_clean_full, y_clean, \"[clean] FE ì œê±° + ê²°ì¸¡/ì´ìƒì¹˜ ì²˜ë¦¬\", dropna=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe1e833",
   "metadata": {},
   "source": [
    "## ğŸ”š Preprocessing v2 ê²°ë¡  ì •ë¦¬\n",
    "\n",
    "- **ìµœì¢… ê¸°ì¤€ ë°ì´í„°**\n",
    "  - **EDA ê¸°ì¤€**: `data/enhanced_data_clean.csv` (8000 Ã— 26)\n",
    "    - êµ¬ì„±: ì›ë³¸ 12ê°œ ì»¬ëŸ¼(`user_id`, `gender`, `age`, `country`, `subscription_type`, `listening_time`, `songs_played_per_day`, `skip_rate`, `device_type`, `ads_listened_per_week`, `offline_listening`, `is_churned`)  \n",
    "      + ê¸°ì¡´ FE 5ê°œ(`engagement_score`, `songs_per_minute`, `skip_intensity`, `skip_rate_cap`, `ads_pressure`)  \n",
    "      + ì‹œê³„ì—´ í”¼ì²˜ 5ê°œ(`listening_time_trend_7d`, `login_frequency_30d`, `days_since_last_login`, `skip_rate_increase_7d`, `freq_of_use_trend_14d`)  \n",
    "      + ê³ ê°ì ‘ì  í”¼ì²˜ 4ê°œ(`customer_support_contact`, `payment_failure_count`, `promotional_email_click`, `app_crash_count_30d`)\n",
    "    - ì—­í• : ì •ì œëœ ì „ì²´ êµ¬ì¡°/ë¶„í¬ ë° ì •ì œ ì „Â·í›„ ë¹„êµë¥¼ ìœ„í•œ **EDA/ë¦¬í¬íŠ¸ìš© ë°ì´í„°**\n",
    "  - **ëª¨ë¸ ì…ë ¥ ê¸°ì¤€**: `data/enhanced_data_clean_model.csv` (8000 Ã— 21)\n",
    "    - êµ¬ì„±: `enhanced_data_clean.csv`ì—ì„œ **FE 5ê°œ ì»¬ëŸ¼(`engagement_score`, `songs_per_minute`, `skip_intensity`, `skip_rate_cap`, `ads_pressure`)ë§Œ ì œê±°**í•œ ë²„ì „  \n",
    "      â†’ ì›ë³¸ ìˆ˜ì¹˜í˜• 6 + ì‹œê³„ì—´ 5 + ê³ ê°ì ‘ì  4 = **ì´ 15ê°œ ìˆ˜ì¹˜ í”¼ì²˜**ë¥¼ ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©\n",
    "\n",
    "- **ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ê·œì¹™ (`enhanced_data_clean.csv` ìƒì„±)**\n",
    "  - `listening_time`, `songs_played_per_day`: ê° ì»¬ëŸ¼ **median**ìœ¼ë¡œ ëŒ€ì²´  \n",
    "    (â†’ ë‘ ì»¬ëŸ¼ì€ ìµœì¢… 15ê°œ ìˆ˜ì¹˜ í”¼ì²˜ì˜ ê¸°ë°˜ì´ ë˜ëŠ” í•µì‹¬ í™œë™ ì§€í‘œì´ë¯€ë¡œ, ê²°ì¸¡ ì‹œ ì¤‘ì•™ê°’ìœ¼ë¡œ ë³´ìˆ˜ì ìœ¼ë¡œ ëŒ€ì²´)\n",
    "  - `payment_failure_count`, `app_crash_count_30d`: NaN â†’ **0íšŒ**ë¡œ ê°„ì£¼\n",
    "  - `customer_support_contact`, `promotional_email_click`: NaN â†’ **False(ì´ë²¤íŠ¸ ì—†ìŒ)** ë¡œ ê°„ì£¼\n",
    "  - ê·¸ ì™¸ ì‹œê³„ì—´ íŠ¸ë Œë“œ/ë³€í™”ìœ¨ ì»¬ëŸ¼(`listening_time_trend_7d`, `freq_of_use_trend_14d`, `skip_rate_increase_7d` ë“±)ì€ í•©ì„± ë‹¨ê³„ì—ì„œ ì´ë¯¸ ê²°ì¸¡ ì—†ìŒ í™•ì¸\n",
    "\n",
    "- **ì´ìƒì¹˜ ì²˜ë¦¬ ê·œì¹™ (`enhanced_data_clean.csv` ìƒì„±)**\n",
    "  - ëŒ€ìƒ: ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼(ë‹¨, `user_id`, `is_churned` ì œì™¸)\n",
    "  - ë°©ë²•: IQR ê¸°ì¤€ìœ¼ë¡œ **[Q1 âˆ’ 1.5Ã—IQR, Q3 + 1.5Ã—IQR]** ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ê°’ì€ í•´ë‹¹ ê²½ê³„ê°’ìœ¼ë¡œ **clip(winsorizing)**\n",
    "\n",
    "- **RandomForest ê¸°ì¤€ ì„±ëŠ¥ ìš”ì•½ (FE ì œê±° + ì‹œê³„ì—´/ê³ ê°ì ‘ì  í¬í•¨, ìµœì¢… 15ê°œ ìˆ˜ì¹˜ í”¼ì²˜)**\n",
    "  - [ì›ë³¸ `enhanced_data.csv` + ê²°ì¸¡ í–‰ drop, 7,527í–‰ ì‚¬ìš©]  \n",
    "    - F1 **0.6151**, AUC **0.8287**  \n",
    "    - ê²°ì¸¡/ê·¹ë‹¨ê°’ì´ ìˆëŠ” í–‰ì„ ì œê±°í•˜ì—¬ **ì˜ˆì¸¡ì´ ì‰¬ìš´ ìƒ˜í”Œ ìœ„ì£¼ì˜ ìƒí•œì„ (upper bound) ì„±ëŠ¥**ìœ¼ë¡œ í•´ì„\n",
    "  - [`enhanced_data_clean.csv` (ê²°ì¸¡/ì´ìƒì¹˜ ì²˜ë¦¬, 8,000í–‰ ì „ë¶€ ì‚¬ìš©) + `enhanced_data_clean_model.csv` (FE ì œê±° í›„ 15ê°œ í”¼ì²˜ ì‚¬ìš©)]  \n",
    "    - F1 **0.5664**, AUC **0.7957**  \n",
    "    - ì •ì œ ê³¼ì •ìœ¼ë¡œ ì¸í•´ F1 ì•½ **âˆ’0.049p(â‰ˆ âˆ’7.9%)**, AUC ì•½ **âˆ’0.033p(â‰ˆ âˆ’4.0%)** ê°ì†Œ  \n",
    "    - ëŒ€ì‹  **ëª¨ë“  ìœ ì €ë¥¼ ì‚¬ìš©**í•˜ê³ , ì´ìƒì¹˜ ì™„í™”ë¡œ **ëª¨ë¸ì˜ ì¼ë°˜í™” ì•ˆì •ì„±**ì„ í™•ë³´\n",
    "\n",
    "- **ìµœì¢… ê²°ì •**\n",
    "  - ëª¨ë¸ í•™ìŠµ/íŠœë‹ ë° ë°°í¬ìš© íŒŒì´í”„ë¼ì¸ì—ì„œëŠ” **FE 5ê°œ(`engagement_score`, `songs_per_minute`, `skip_intensity`, `skip_rate_cap`, `ads_pressure`)ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ **,  \n",
    "    ì›ë³¸ ìˆ˜ì¹˜í˜• 6 + ì‹œê³„ì—´ 5 + ê³ ê°ì ‘ì  4 = **15ê°œ ìˆ˜ì¹˜ í”¼ì²˜ë§Œ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©**í•œë‹¤.\n",
    "  - EDA ë…¸íŠ¸ë¶(`eda_add.ipynb`)ì—ì„œëŠ” `enhanced_data.csv` / `enhanced_data_clean.csv`ë¥¼ í™œìš©í•´  \n",
    "    ê²°ì¸¡/ì´ìƒì¹˜ ì²˜ë¦¬ ì „Â·í›„ì˜ ë¶„í¬ ì°¨ì´ì™€ ì‹œê³„ì—´/ê³ ê°ì ‘ì  í”¼ì²˜ì˜ íš¨ê³¼ë¥¼ ì„¤ëª…í•œë‹¤.\n",
    "  - ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì€ **`enhanced_data_clean_model.csv` + 15ê°œ í”¼ì²˜ë¥¼ ê³µí†µ ê¸°ì¤€**ìœ¼ë¡œ ì§„í–‰í•œë‹¤.\n",
    "\n",
    "- **ëª¨ë¸ìš© ì¶•ì†Œ ë°ì´í„° (FE ì œê±° CSV)**\n",
    "  - íŒŒì¼: `data/enhanced_data_clean_model.csv`\n",
    "  - êµ¬ì„±: `enhanced_data_clean.csv`ì—ì„œ **FE 5ê°œ ì»¬ëŸ¼ë§Œ ì œê±°**í•œ ë²„ì „\n",
    "  - ì œê±°ëœ ì»¬ëŸ¼: `engagement_score`, `songs_per_minute`, `skip_intensity`, `skip_rate_cap`, `ads_pressure`\n",
    "  - ìš©ë„: ìµœì¢… ëª¨ë¸ í•™ìŠµ/íŠœë‹ ë° ëª¨ë¸ ê°„ ì„±ëŠ¥ ë¹„êµë¥¼ ìœ„í•œ ê³µí†µ ì…ë ¥ ë°ì´í„°ë¡œ ì‚¬ìš©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d6d00d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸ clean shape: (8000, 26)\n",
      "ëª¨ë¸ìš© shape (FE 5ê°œ ì œê±° í›„): (8000, 21)\n",
      "ëª¨ë¸ìš© CSV ì €ì¥ ì™„ë£Œ: c:\\2nd_project\\SKN21-2nd-2Team\\data\\enhanced_data_clean_model.csv\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ìš© CSV: FE 5ê°œë¥¼ ì œê±°í•œ ë²„ì „ ì €ì¥\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "CLEAN_PATH = PROJECT_ROOT / \"data\" / \"enhanced_data_clean.csv\"\n",
    "MODEL_PATH = PROJECT_ROOT / \"data\" / \"enhanced_data_clean_model.csv\"\n",
    "\n",
    "fe_cols = [\n",
    "    \"engagement_score\",\n",
    "    \"songs_per_minute\",\n",
    "    \"skip_intensity\",\n",
    "    \"skip_rate_cap\",\n",
    "    \"ads_pressure\",\n",
    "]\n",
    "\n",
    "df_full = pd.read_csv(CLEAN_PATH)\n",
    "print(\"ì›ë³¸ clean shape:\", df_full.shape)\n",
    "\n",
    "keep_df = df_full.drop(columns=fe_cols, errors=\"ignore\")\n",
    "print(\"ëª¨ë¸ìš© shape (FE 5ê°œ ì œê±° í›„):\", keep_df.shape)\n",
    "\n",
    "keep_df.to_csv(MODEL_PATH, index=False)\n",
    "print(f\"ëª¨ë¸ìš© CSV ì €ì¥ ì™„ë£Œ: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ac5487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7b23a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
