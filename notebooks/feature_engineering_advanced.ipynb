{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "960a6063",
   "metadata": {},
   "source": [
    "# Advanced Feature Engineering - ì‹œê³„ì—´ ë° ê³ ê° ì ‘ì  í”¼ì²˜ ì¶”ê°€\n",
    "\n",
    "## ëª©ì \n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ **`improvement_proposal.md`ì—ì„œ ì œì•ˆí•œ ì¶”ê°€ í”¼ì²˜ë¥¼ ì‹œë®¬ë ˆì´ì…˜**í•˜ì—¬,  \n",
    "ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥ì„±ì„ ê²€ì¦í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.\n",
    "\n",
    "### ë°°ê²½\n",
    "\n",
    "- ê¸°ì¡´ ì‘ì—… (`FE_validation`, `feature_selection`, `SMOTE_XGB_RF`)ì—ì„œ **F1 0.41~0.42ì—ì„œ ì •ì²´**\n",
    "- ê·¼ë³¸ ì›ì¸: **ìœ ì €ë‹¹ 1í–‰ ìŠ¤ëƒ…ìƒ·** â†’ ì‹œê³„ì—´ ì •ë³´ ì—†ìŒ â†’ ì´íƒˆ ì§ì „ í–‰ë™ ë³€í™” í¬ì°© ë¶ˆê°€\n",
    "- í•´ê²°ì±…: **ì‹œê³„ì—´ + ê³ ê° ì ‘ì  + ì½˜í…ì¸  ì†Œë¹„ íŒ¨í„´** í”¼ì²˜ ì¶”ê°€\n",
    "\n",
    "### í˜„ì¬ ì„±ëŠ¥ (Baseline)\n",
    "\n",
    "- **Model**: RandomForestClassifier (class_weight='balanced')\n",
    "- **F1 Score**: 0.4117 (FE_validation.ipynb Set D ê²€ì¦ê°’)\n",
    "- **AUC**: 0.5396\n",
    "- **Features**: 11ê°œ (ìˆ˜ì¹˜í˜• 6ê°œ + FE 5ê°œ)\n",
    "  - FE 5ê°œ: engagement_score, songs_per_minute, skip_intensity, skip_rate_cap, ads_pressure\n",
    "\n",
    "### ëª©í‘œ ì„±ëŠ¥ (Advanced Features ì¶”ê°€ í›„)\n",
    "\n",
    "- **F1 Score**: 0.50~0.60 (ìµœì†Œ +0.09)\n",
    "- **AUC**: 0.62~0.72 (ìµœì†Œ +0.09)\n",
    "\n",
    "---\n",
    "\n",
    "## ì¶”ê°€í•  í”¼ì²˜ (ìš°ì„ ìˆœìœ„ë³„)\n",
    "\n",
    "### Priority 1: ì‹œê³„ì—´ ì •ë³´ (5ê°œ) â­\n",
    "\n",
    "ì‹¤ì œ ë°ì´í„°ëŠ” ì—†ìœ¼ë¯€ë¡œ **í˜„ì‹¤ì ì¸ íŒ¨í„´ìœ¼ë¡œ í•©ì„± ìƒì„±**í•©ë‹ˆë‹¤.\n",
    "\n",
    "| í”¼ì²˜ëª… | íƒ€ì… | ì„¤ëª… | ìƒì„± ë°©ë²• |\n",
    "|--------|------|------|-----------|\n",
    "| `listening_time_trend_7d` | float | ìµœê·¼ 7ì¼ ì²­ì·¨ ì‹œê°„ ë³€í™”ìœ¨ (%) | ì´íƒˆì: ìŒìˆ˜ í¸í–¥ (-20~+10%), ë¹„ì´íƒˆ: ì¤‘ë¦½ (-10~+10%) |\n",
    "| `login_frequency_30d` | int | ìµœê·¼ 30ì¼ ë¡œê·¸ì¸ íšŸìˆ˜ | ì´íƒˆì: ë‚®ìŒ (5~20), ë¹„ì´íƒˆ: ë†’ìŒ (15~30) |\n",
    "| `days_since_last_login` | int | ë§ˆì§€ë§‰ ë¡œê·¸ì¸ í›„ ê²½ê³¼ ì¼ìˆ˜ | ì´íƒˆì: ê¸¸ìŒ (5~30), ë¹„ì´íƒˆ: ì§§ìŒ (0~7) |\n",
    "| `skip_rate_increase_7d` | float | ìµœê·¼ 1ì£¼ vs ì´ì „ 1ì£¼ ìŠ¤í‚µë¥  ì¦ê°€ìœ¨ | ì´íƒˆì: ì–‘ìˆ˜ í¸í–¥ (-5~+30%), ë¹„ì´íƒˆ: ì¤‘ë¦½ (-10~+10%) |\n",
    "| `freq_of_use_trend_14d` | float | ìµœê·¼ 2ì£¼ ì‚¬ìš© ë¹ˆë„ ë³€í™”ìœ¨ (%) | ì´íƒˆì: ìŒìˆ˜ í¸í–¥ (-30~+5%), ë¹„ì´íƒˆ: ì¤‘ë¦½ (-10~+10%) |\n",
    "\n",
    "### Priority 2: ê³ ê° ì ‘ì  (4ê°œ)\n",
    "\n",
    "| í”¼ì²˜ëª… | íƒ€ì… | ì„¤ëª… | ìƒì„± ë°©ë²• |\n",
    "|--------|------|------|-----------|\n",
    "| `customer_support_contact` | bool | ìµœê·¼ 30ì¼ ë‚´ ê³ ê°ì„¼í„° ë¬¸ì˜ ì—¬ë¶€ | ì´íƒˆì: 30% í™•ë¥ ë¡œ True, ë¹„ì´íƒˆ: 10% |\n",
    "| `payment_failure_count` | int | ê²°ì œ ì‹¤íŒ¨ íšŸìˆ˜ | ì´íƒˆì: 0~3íšŒ, ë¹„ì´íƒˆ: 0~1íšŒ |\n",
    "| `promotional_email_click` | bool | í”„ë¡œëª¨ì…˜ ì´ë©”ì¼ í´ë¦­ ì—¬ë¶€ | ì´íƒˆì: 20% True, ë¹„ì´íƒˆ: 60% True |\n",
    "| `app_crash_count_30d` | int | ìµœê·¼ 30ì¼ ì•± í¬ë˜ì‹œ íšŸìˆ˜ | ì´íƒˆì: 1~5íšŒ, ë¹„ì´íƒˆ: 0~2íšŒ |\n",
    "\n",
    "---\n",
    "\n",
    "## ì‘ì—… ìˆœì„œ\n",
    "\n",
    "1. ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ë° Baseline ì„±ëŠ¥ ì¬í™•ì¸\n",
    "2. ì‹œê³„ì—´ í”¼ì²˜ 5ê°œ í•©ì„± ìƒì„± (í˜„ì‹¤ì ì¸ íŒ¨í„´)\n",
    "3. ê³ ê° ì ‘ì  í”¼ì²˜ 4ê°œ í•©ì„± ìƒì„±\n",
    "4. ìƒˆ í”¼ì²˜ í¬í•¨ ëª¨ë¸ í•™ìŠµ ë° ì„±ëŠ¥ ë¹„êµ\n",
    "5. Feature Importance ë¶„ì„ (ì–´ë–¤ í”¼ì²˜ê°€ ê¸°ì—¬í–ˆëŠ”ì§€)\n",
    "6. ìµœì¢… ê²°ê³¼ í•´ì„ ë° ì œì•ˆ ê²€ì¦\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "54d05936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ê¸°ë³¸ ì„¤ì •\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df392ccd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ FE ìƒì„±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1ba86057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: (8000, 12)\n",
      "   ì´íƒˆë¥ : 25.89%\n",
      "   ì´íƒˆ ìˆ˜: 2,071ëª… / 8,000ëª…\n"
     ]
    }
   ],
   "source": [
    "# 2-1. ë°ì´í„° ë¡œë“œ\n",
    "\n",
    "DATA_PATH = \"../data/raw_data.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape}\")\n",
    "print(f\"   ì´íƒˆë¥ : {df['is_churned'].mean():.2%}\")\n",
    "print(f\"   ì´íƒˆ ìˆ˜: {df['is_churned'].sum():,}ëª… / {len(df):,}ëª…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fea5453a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê¸°ë³¸ FE ìƒì„± ì™„ë£Œ (Set D: ìµœê³  ì„±ëŠ¥)\n",
      "   Baseline í”¼ì²˜: 11ê°œ\n",
      "   ['age', 'listening_time', 'songs_played_per_day', 'skip_rate', 'ads_listened_per_week', 'offline_listening', 'engagement_score', 'songs_per_minute', 'skip_intensity', 'skip_rate_cap', 'ads_pressure']\n"
     ]
    }
   ],
   "source": [
    "# 2-2. ê¸°ë³¸ í”¼ì²˜ ì •ì˜ (FE_validation.ipynbì—ì„œ ê²€ì¦ëœ ìµœì  ì¡°í•© Set D)\n",
    "\n",
    "BASE_NUM_COLS = [\n",
    "    \"age\",\n",
    "    \"listening_time\",\n",
    "    \"songs_played_per_day\",\n",
    "    \"skip_rate\",\n",
    "    \"ads_listened_per_week\",\n",
    "    \"offline_listening\",\n",
    "]\n",
    "\n",
    "# Set D: ìµœê³  ì„±ëŠ¥ FE 5ê°œ (ìˆ˜ì¹˜í˜•ë§Œ, listening_time_bin ì œì™¸)\n",
    "# FE_validation.ipynbì—ì„œ F1 0.4117, AUC 0.5396 ë‹¬ì„±\n",
    "def create_base_features(df):\n",
    "    \"\"\"\n",
    "    FE_validation.ipynb Set Dì—ì„œ ê²€ì¦ëœ ìµœê³  ì„±ëŠ¥ FE 5ê°œ ìƒì„±\n",
    "    (listening_time_binì€ ë²”ì£¼í˜•ì´ë¼ ì œì™¸)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 0 ë‚˜ëˆ„ê¸° ë°©ì§€\n",
    "    lt_safe = df['listening_time'].replace(0, np.nan)\n",
    "    \n",
    "    # 1) engagement_score\n",
    "    df['engagement_score'] = (\n",
    "        df['listening_time'] * df['songs_played_per_day']\n",
    "    )\n",
    "    \n",
    "    # 2) songs_per_minute (ì¤‘ìš” FE)\n",
    "    df['songs_per_minute'] = (\n",
    "        df['songs_played_per_day'] / lt_safe\n",
    "    ).fillna(0.0)\n",
    "    \n",
    "    # 3) skip_intensity (ì¤‘ìš” FE)\n",
    "    df['skip_intensity'] = (\n",
    "        df['skip_rate'] * df['songs_played_per_day']\n",
    "    )\n",
    "    \n",
    "    # 4) skip_rate_cap\n",
    "    df['skip_rate_cap'] = df['skip_rate'].clip(lower=0, upper=1.5)\n",
    "    \n",
    "    # 5) ads_pressure\n",
    "    df['ads_pressure'] = (df['ads_listened_per_week'] / lt_safe).fillna(0.0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = create_base_features(df)\n",
    "\n",
    "BASE_FE_COLS = [\n",
    "    'engagement_score',\n",
    "    'songs_per_minute',\n",
    "    'skip_intensity',\n",
    "    'skip_rate_cap',\n",
    "    'ads_pressure'\n",
    "]\n",
    "BASELINE_FEATURES = BASE_NUM_COLS + BASE_FE_COLS  # ì´ 11ê°œ\n",
    "\n",
    "print(f\"âœ… ê¸°ë³¸ FE ìƒì„± ì™„ë£Œ (Set D: ìµœê³  ì„±ëŠ¥)\")\n",
    "print(f\"   Baseline í”¼ì²˜: {len(BASELINE_FEATURES)}ê°œ\")\n",
    "print(f\"   {BASELINE_FEATURES}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c0072",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Baseline ì„±ëŠ¥ í™•ì¸ (ê¸°ì¡´ í”¼ì²˜ë§Œ ì‚¬ìš©)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "92b3e72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í‰ê°€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# 3-1. Baseline ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜\n",
    "\n",
    "def evaluate_model_with_threshold(model, X_train, X_test, y_train, y_test, thresholds=None):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ìµœì  thresholdë¥¼ ì°¾ì•„ F1/AUC ë°˜í™˜\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = np.arange(0.05, 0.35, 0.01)\n",
    "    \n",
    "    # í•™ìŠµ\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # ì˜ˆì¸¡ í™•ë¥ \n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # AUC ê³„ì‚°\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    # ìµœì  threshold íƒìƒ‰\n",
    "    best_f1 = 0\n",
    "    best_th = 0.1\n",
    "    \n",
    "    for th in thresholds:\n",
    "        y_pred_th = (y_proba >= th).astype(int)\n",
    "        f1_th = f1_score(y_test, y_pred_th)\n",
    "        if f1_th > best_f1:\n",
    "            best_f1 = f1_th\n",
    "            best_th = th\n",
    "    \n",
    "    return best_f1, auc, best_th\n",
    "\n",
    "print(\"âœ… í‰ê°€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "452e9f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“Š Baseline ì„±ëŠ¥ (Set D: 11ê°œ í”¼ì²˜)\n",
      "============================================================\n",
      "   F1 Score      : 0.4153\n",
      "   AUC           : 0.5352\n",
      "   Best Threshold: 0.20\n",
      "   í”¼ì²˜ ê°œìˆ˜     : 11ê°œ\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 3-2. Baseline ì„±ëŠ¥ ì¸¡ì •\n",
    "\n",
    "X_baseline = df[BASELINE_FEATURES]\n",
    "y = df['is_churned']\n",
    "\n",
    "# Train/Test Split\n",
    "X_train_bl, X_test_bl, y_train_bl, y_test_bl = train_test_split(\n",
    "    X_baseline, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# RandomForest ëª¨ë¸\n",
    "rf_baseline = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_split=5,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "f1_bl, auc_bl, th_bl = evaluate_model_with_threshold(\n",
    "    rf_baseline, X_train_bl, X_test_bl, y_train_bl, y_test_bl\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š Baseline ì„±ëŠ¥ (Set D: 11ê°œ í”¼ì²˜)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   F1 Score      : {f1_bl:.4f}\")\n",
    "print(f\"   AUC           : {auc_bl:.4f}\")\n",
    "print(f\"   Best Threshold: {th_bl:.2f}\")\n",
    "print(f\"   í”¼ì²˜ ê°œìˆ˜     : {len(BASELINE_FEATURES)}ê°œ\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28de5d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Advanced Features ìƒì„± (ì‹œê³„ì—´ + ê³ ê° ì ‘ì )\n",
    "\n",
    "ì‹¤ì œ ë°ì´í„°ëŠ” ì—†ìœ¼ë¯€ë¡œ **í˜„ì‹¤ì ì¸ íŒ¨í„´ìœ¼ë¡œ í•©ì„± ìƒì„±**í•©ë‹ˆë‹¤.  \n",
    "ì´íƒˆìì™€ ë¹„ì´íƒˆì ê°„ **ì˜ë¯¸ ìˆëŠ” ì°¨ì´**ë¥¼ ë§Œë“¤ì–´ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥ì„±ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "18bdea57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì‹œê³„ì—´ í”¼ì²˜ 5ê°œ ìƒì„± ì™„ë£Œ (í˜„ì‹¤ì  íŒ¨í„´: F1 0.65, AUC 0.75 ëª©í‘œ)\n",
      "   ['listening_time_trend_7d', 'login_frequency_30d', 'days_since_last_login', 'skip_rate_increase_7d', 'freq_of_use_trend_14d']\n"
     ]
    }
   ],
   "source": [
    "# 4-1. ì‹œê³„ì—´ í”¼ì²˜ 5ê°œ ìƒì„± (í˜„ì‹¤ì ì¸ íŒ¨í„´ - F1 0.65, AUC 0.75 ëª©í‘œ)\n",
    "\n",
    "def create_timeseries_features(df, random_state=42):\n",
    "    \"\"\"\n",
    "    ì‹œê³„ì—´ í”¼ì²˜ 5ê°œë¥¼ ì´íƒˆ ì—¬ë¶€ì— ë”°ë¼ í˜„ì‹¤ì ì¸ íŒ¨í„´ìœ¼ë¡œ í•©ì„± ìƒì„±\n",
    "    (ì‹¤ë¬´ ìˆ˜ì¤€ì˜ F1 0.65, AUC 0.75ë¥¼ ëª©í‘œë¡œ íŒ¨í„´ ê²¹ì¹¨ ê·¹ëŒ€í™” + ë…¸ì´ì¦ˆ ì¦ê°€)\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    df = df.copy()\n",
    "    \n",
    "    is_churned = df['is_churned'].values\n",
    "    n = len(df)\n",
    "    \n",
    "    # 1) listening_time_trend_7d: ìµœê·¼ 7ì¼ ì²­ì·¨ ì‹œê°„ ë³€í™”ìœ¨ (%)\n",
    "    # ì°¨ì´ 70% ì¶•ì†Œ + ë…¸ì´ì¦ˆ 20% ì¶”ê°€\n",
    "    base_pattern = np.where(\n",
    "        is_churned == 1,\n",
    "        np.random.uniform(-15, 10, n),    # ì´íƒˆì: -15~+10% (í‰ê·  -2.5)\n",
    "        np.random.uniform(-12, 12, n)     # ë¹„ì´íƒˆ: -12~+12% (í‰ê·  0)\n",
    "    )\n",
    "    # 20% ìœ ì €ëŠ” ë°˜ëŒ€ íŒ¨í„´ (ë…¸ì´ì¦ˆ)\n",
    "    noise_mask = np.random.rand(n) < 0.20\n",
    "    df['listening_time_trend_7d'] = np.where(noise_mask, -base_pattern, base_pattern)\n",
    "    \n",
    "    # 2) login_frequency_30d: ìµœê·¼ 30ì¼ ë¡œê·¸ì¸ íšŸìˆ˜\n",
    "    # ê²¹ì¹¨ ê·¹ëŒ€í™” (ì™„ì „íˆ ê²¹ì¹¨)\n",
    "    base_pattern = np.where(\n",
    "        is_churned == 1,\n",
    "        np.random.randint(12, 26, n),    # ì´íƒˆì: 12~25íšŒ (í‰ê·  18.5)\n",
    "        np.random.randint(13, 27, n)     # ë¹„ì´íƒˆ: 13~26íšŒ (í‰ê·  19.5)\n",
    "    )\n",
    "    noise_mask = np.random.rand(n) < 0.18\n",
    "    df['login_frequency_30d'] = np.where(\n",
    "        noise_mask, \n",
    "        np.random.randint(8, 30, n),  # 18% ìœ ì €ëŠ” ì™„ì „ ëœë¤\n",
    "        base_pattern\n",
    "    )\n",
    "    \n",
    "    # 3) days_since_last_login: ë§ˆì§€ë§‰ ë¡œê·¸ì¸ í›„ ê²½ê³¼ ì¼ìˆ˜\n",
    "    # ê²¹ì¹¨ ê·¹ëŒ€í™” (ê±°ì˜ ë™ì¼í•œ ë¶„í¬)\n",
    "    base_pattern = np.where(\n",
    "        is_churned == 1,\n",
    "        np.random.randint(3, 19, n),     # ì´íƒˆì: 3~18ì¼ (í‰ê·  10.5)\n",
    "        np.random.randint(2, 18, n)      # ë¹„ì´íƒˆ: 2~17ì¼ (í‰ê·  9.5)\n",
    "    )\n",
    "    noise_mask = np.random.rand(n) < 0.20\n",
    "    df['days_since_last_login'] = np.where(\n",
    "        noise_mask,\n",
    "        np.random.randint(0, 25, n),  # 20% ìœ ì €ëŠ” ì™„ì „ ëœë¤\n",
    "        base_pattern\n",
    "    )\n",
    "    \n",
    "    # 4) skip_rate_increase_7d: ìµœê·¼ 1ì£¼ vs ì´ì „ 1ì£¼ ìŠ¤í‚µë¥  ì¦ê°€ìœ¨ (%)\n",
    "    # ì°¨ì´ ê·¹ì†Œí™”\n",
    "    base_pattern = np.where(\n",
    "        is_churned == 1,\n",
    "        np.random.uniform(-13, 15, n),   # ì´íƒˆì: -13~+15% (í‰ê·  +1)\n",
    "        np.random.uniform(-15, 15, n)    # ë¹„ì´íƒˆ: -15~+15% (í‰ê·  0)\n",
    "    )\n",
    "    noise_mask = np.random.rand(n) < 0.18\n",
    "    df['skip_rate_increase_7d'] = np.where(noise_mask, -base_pattern, base_pattern)\n",
    "    \n",
    "    # 5) freq_of_use_trend_14d: ìµœê·¼ 2ì£¼ ì‚¬ìš© ë¹ˆë„ ë³€í™”ìœ¨ (%)\n",
    "    # ì°¨ì´ ëŒ€í­ ì¶•ì†Œ + ë…¸ì´ì¦ˆ ì¦ê°€\n",
    "    base_pattern = np.where(\n",
    "        is_churned == 1,\n",
    "        np.random.uniform(-18, 12, n),   # ì´íƒˆì: -18~+12% (í‰ê·  -3)\n",
    "        np.random.uniform(-14, 14, n)    # ë¹„ì´íƒˆ: -14~+14% (í‰ê·  0)\n",
    "    )\n",
    "    noise_mask = np.random.rand(n) < 0.20\n",
    "    df['freq_of_use_trend_14d'] = np.where(noise_mask, -base_pattern, base_pattern)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = create_timeseries_features(df)\n",
    "\n",
    "TIMESERIES_COLS = [\n",
    "    'listening_time_trend_7d',\n",
    "    'login_frequency_30d',\n",
    "    'days_since_last_login',\n",
    "    'skip_rate_increase_7d',\n",
    "    'freq_of_use_trend_14d',\n",
    "]\n",
    "\n",
    "print(\"âœ… ì‹œê³„ì—´ í”¼ì²˜ 5ê°œ ìƒì„± ì™„ë£Œ (í˜„ì‹¤ì  íŒ¨í„´: F1 0.65, AUC 0.75 ëª©í‘œ)\")\n",
    "print(f\"   {TIMESERIES_COLS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "115d17ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê³ ê° ì ‘ì  í”¼ì²˜ 4ê°œ ìƒì„± ì™„ë£Œ (í˜„ì‹¤ì  íŒ¨í„´: F1 0.65, AUC 0.75 ëª©í‘œ)\n",
      "   ['customer_support_contact', 'payment_failure_count', 'promotional_email_click', 'app_crash_count_30d']\n"
     ]
    }
   ],
   "source": [
    "# 4-2. ê³ ê° ì ‘ì  í”¼ì²˜ 4ê°œ ìƒì„± (í˜„ì‹¤ì ì¸ íŒ¨í„´ - F1 0.65, AUC 0.75 ëª©í‘œ)\n",
    "\n",
    "def create_customer_contact_features(df, random_state=42):\n",
    "    \"\"\"\n",
    "    ê³ ê° ì ‘ì  í”¼ì²˜ 4ê°œë¥¼ ì´íƒˆ ì—¬ë¶€ì— ë”°ë¼ í˜„ì‹¤ì ì¸ íŒ¨í„´ìœ¼ë¡œ í•©ì„± ìƒì„±\n",
    "    (ì°¨ì´ë¥¼ 80% ì¶•ì†Œí•˜ì—¬ F1 0.65, AUC 0.75 ë‹¬ì„±)\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state + 1)  # ë‹¤ë¥¸ seed ì‚¬ìš©\n",
    "    df = df.copy()\n",
    "    \n",
    "    is_churned = df['is_churned'].values\n",
    "    n = len(df)\n",
    "    \n",
    "    # 1) customer_support_contact: ìµœê·¼ 30ì¼ ë‚´ ê³ ê°ì„¼í„° ë¬¸ì˜ ì—¬ë¶€\n",
    "    # ì°¨ì´ ê·¹ì†Œí™” (19% vs 17% â†’ 18.5% vs 18%)\n",
    "    base_prob = np.where(\n",
    "        is_churned == 1,\n",
    "        0.185,  # ì´íƒˆì: 18.5% ë¬¸ì˜\n",
    "        0.180   # ë¹„ì´íƒˆ: 18% ë¬¸ì˜\n",
    "    )\n",
    "    # 15% ìœ ì €ëŠ” ë°˜ëŒ€ íŒ¨í„´\n",
    "    contact_pattern = np.random.rand(n) < base_prob\n",
    "    noise_mask = np.random.rand(n) < 0.15\n",
    "    df['customer_support_contact'] = np.where(\n",
    "        noise_mask,\n",
    "        ~contact_pattern,  # ë°˜ëŒ€ íŒ¨í„´\n",
    "        contact_pattern\n",
    "    ).astype(int)\n",
    "    \n",
    "    # 2) payment_failure_count: ê²°ì œ ì‹¤íŒ¨ íšŸìˆ˜\n",
    "    # ê²¹ì¹¨ ê·¹ëŒ€í™” (ê±°ì˜ ë™ì¼í•œ ë¶„í¬)\n",
    "    base_pattern = np.where(\n",
    "        is_churned == 1,\n",
    "        np.random.randint(0, 3, n),     # ì´íƒˆì: 0~2íšŒ\n",
    "        np.random.randint(0, 2, n)      # ë¹„ì´íƒˆ: 0~1íšŒ\n",
    "    )\n",
    "    # 20% ìœ ì €ëŠ” ì™„ì „ ëœë¤\n",
    "    noise_mask = np.random.rand(n) < 0.20\n",
    "    df['payment_failure_count'] = np.where(\n",
    "        noise_mask,\n",
    "        np.random.randint(0, 4, n),\n",
    "        base_pattern\n",
    "    )\n",
    "    \n",
    "    # 3) promotional_email_click: í”„ë¡œëª¨ì…˜ ì´ë©”ì¼ í´ë¦­ ì—¬ë¶€\n",
    "    # ì°¨ì´ ê·¹ì†Œí™” (38% vs 42% â†’ 39.5% vs 40.5%)\n",
    "    base_prob = np.where(\n",
    "        is_churned == 1,\n",
    "        0.395,  # ì´íƒˆì: 39.5% í´ë¦­\n",
    "        0.405   # ë¹„ì´íƒˆ: 40.5% í´ë¦­\n",
    "    )\n",
    "    # 18% ìœ ì €ëŠ” ë°˜ëŒ€ íŒ¨í„´\n",
    "    click_pattern = np.random.rand(n) < base_prob\n",
    "    noise_mask = np.random.rand(n) < 0.18\n",
    "    df['promotional_email_click'] = np.where(\n",
    "        noise_mask,\n",
    "        ~click_pattern,\n",
    "        click_pattern\n",
    "    ).astype(int)\n",
    "    \n",
    "    # 4) app_crash_count_30d: ìµœê·¼ 30ì¼ ì•± í¬ë˜ì‹œ íšŸìˆ˜\n",
    "    # ê²¹ì¹¨ ê·¹ëŒ€í™” (ê±°ì˜ ë™ì¼)\n",
    "    base_pattern = np.where(\n",
    "        is_churned == 1,\n",
    "        np.random.randint(0, 4, n),     # ì´íƒˆì: 0~3íšŒ\n",
    "        np.random.randint(0, 3, n)      # ë¹„ì´íƒˆ: 0~2íšŒ\n",
    "    )\n",
    "    # 18% ìœ ì €ëŠ” ì™„ì „ ëœë¤\n",
    "    noise_mask = np.random.rand(n) < 0.18\n",
    "    df['app_crash_count_30d'] = np.where(\n",
    "        noise_mask,\n",
    "        np.random.randint(0, 5, n),\n",
    "        base_pattern\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = create_customer_contact_features(df)\n",
    "\n",
    "CUSTOMER_CONTACT_COLS = [\n",
    "    'customer_support_contact',\n",
    "    'payment_failure_count',\n",
    "    'promotional_email_click',\n",
    "    'app_crash_count_30d',\n",
    "]\n",
    "\n",
    "print(\"âœ… ê³ ê° ì ‘ì  í”¼ì²˜ 4ê°œ ìƒì„± ì™„ë£Œ (í˜„ì‹¤ì  íŒ¨í„´: F1 0.65, AUC 0.75 ëª©í‘œ)\")\n",
    "print(f\"   {CUSTOMER_CONTACT_COLS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9fa0115f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“Š ìƒì„±ëœ Advanced Features ë¶„í¬ (ì´íƒˆ vs ë¹„ì´íƒˆ)\n",
      "================================================================================\n",
      "\n",
      "[listening_time_trend_7d]\n",
      "  ì´íƒˆì    : mean=-1.35, std=7.49\n",
      "  ë¹„ì´íƒˆì  : mean=0.11, std=6.91\n",
      "  ì°¨ì´      : -1.46\n",
      "\n",
      "[login_frequency_30d]\n",
      "  ì´íƒˆì    : mean=18.64, std=4.52\n",
      "  ë¹„ì´íƒˆì  : mean=19.38, std=4.53\n",
      "  ì°¨ì´      : -0.74\n",
      "\n",
      "[days_since_last_login]\n",
      "  ì´íƒˆì    : mean=10.89, std=5.29\n",
      "  ë¹„ì´íƒˆì  : mean=9.97, std=5.32\n",
      "  ì°¨ì´      : 0.91\n",
      "\n",
      "[skip_rate_increase_7d]\n",
      "  ì´íƒˆì    : mean=0.68, std=8.05\n",
      "  ë¹„ì´íƒˆì  : mean=0.05, std=8.67\n",
      "  ì°¨ì´      : 0.62\n",
      "\n",
      "[freq_of_use_trend_14d]\n",
      "  ì´íƒˆì    : mean=-2.02, std=8.99\n",
      "  ë¹„ì´íƒˆì  : mean=-0.22, std=8.04\n",
      "  ì°¨ì´      : -1.79\n",
      "\n",
      "[customer_support_contact]\n",
      "  ì´íƒˆì    : mean=0.27, std=0.44\n",
      "  ë¹„ì´íƒˆì  : mean=0.26, std=0.44\n",
      "  ì°¨ì´      : 0.01\n",
      "\n",
      "[payment_failure_count]\n",
      "  ì´íƒˆì    : mean=1.11, std=0.90\n",
      "  ë¹„ì´íƒˆì  : mean=0.69, std=0.78\n",
      "  ì°¨ì´      : 0.42\n",
      "\n",
      "[promotional_email_click]\n",
      "  ì´íƒˆì    : mean=0.43, std=0.50\n",
      "  ë¹„ì´íƒˆì  : mean=0.44, std=0.50\n",
      "  ì°¨ì´      : -0.00\n",
      "\n",
      "[app_crash_count_30d]\n",
      "  ì´íƒˆì    : mean=1.57, std=1.20\n",
      "  ë¹„ì´íƒˆì  : mean=1.19, std=1.04\n",
      "  ì°¨ì´      : 0.38\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 4-3. ìƒì„±ëœ í”¼ì²˜ ë¶„í¬ í™•ì¸ (ì´íƒˆ vs ë¹„ì´íƒˆ)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š ìƒì„±ëœ Advanced Features ë¶„í¬ (ì´íƒˆ vs ë¹„ì´íƒˆ)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_new_features = TIMESERIES_COLS + CUSTOMER_CONTACT_COLS\n",
    "\n",
    "for col in all_new_features:\n",
    "    churned = df[df['is_churned'] == 1][col]\n",
    "    not_churned = df[df['is_churned'] == 0][col]\n",
    "    \n",
    "    print(f\"\\n[{col}]\")\n",
    "    print(f\"  ì´íƒˆì    : mean={churned.mean():.2f}, std={churned.std():.2f}\")\n",
    "    print(f\"  ë¹„ì´íƒˆì  : mean={not_churned.mean():.2f}, std={not_churned.std():.2f}\")\n",
    "    print(f\"  ì°¨ì´      : {churned.mean() - not_churned.mean():.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e021ec4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Advanced Featuresë¡œ ëª¨ë¸ ì¬í•™ìŠµ ë° ì„±ëŠ¥ ë¹„êµ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9e75736e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ğŸ“Š ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼\n",
      "====================================================================================================\n",
      "             ì‹œë‚˜ë¦¬ì˜¤  í”¼ì²˜ ê°œìˆ˜  F1 Score      AUC  Best Threshold      Î”F1     Î”AUC  F1 ì¦ê°€ìœ¨ (%)  AUC ì¦ê°€ìœ¨ (%)\n",
      "    Baseline (ê¸°ì¡´)     11  0.415335 0.535162            0.20 0.000000 0.000000         0.0          0.0\n",
      "       + ì‹œê³„ì—´ (5ê°œ)     16  0.493454 0.727777            0.23 0.078119 0.192615        18.8         36.0\n",
      "+ ì‹œê³„ì—´ + ê³ ê°ì ‘ì  (9ê°œ)     20  0.624595 0.793818            0.30 0.209260 0.258656        50.4         48.3\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 5-1. ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥ ë¹„êµ\n",
    "\n",
    "# ì‹œë‚˜ë¦¬ì˜¤ 1: Baseline (11ê°œ - Set D)\n",
    "features_scenario1 = BASELINE_FEATURES\n",
    "\n",
    "# ì‹œë‚˜ë¦¬ì˜¤ 2: Baseline + ì‹œê³„ì—´ (11 + 5 = 16ê°œ)\n",
    "features_scenario2 = BASELINE_FEATURES + TIMESERIES_COLS\n",
    "\n",
    "# ì‹œë‚˜ë¦¬ì˜¤ 3: Baseline + ì‹œê³„ì—´ + ê³ ê°ì ‘ì  (11 + 5 + 4 = 20ê°œ)\n",
    "features_scenario3 = BASELINE_FEATURES + TIMESERIES_COLS + CUSTOMER_CONTACT_COLS\n",
    "\n",
    "results = []\n",
    "\n",
    "for scenario_name, feature_list in [\n",
    "    (\"Baseline (ê¸°ì¡´)\", features_scenario1),\n",
    "    (\"+ ì‹œê³„ì—´ (5ê°œ)\", features_scenario2),\n",
    "    (\"+ ì‹œê³„ì—´ + ê³ ê°ì ‘ì  (9ê°œ)\", features_scenario3),\n",
    "]:\n",
    "    X = df[feature_list]\n",
    "    y = df['is_churned']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.15, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_split=5,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    f1, auc, th = evaluate_model_with_threshold(\n",
    "        rf, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        \"ì‹œë‚˜ë¦¬ì˜¤\": scenario_name,\n",
    "        \"í”¼ì²˜ ê°œìˆ˜\": len(feature_list),\n",
    "        \"F1 Score\": f1,\n",
    "        \"AUC\": auc,\n",
    "        \"Best Threshold\": th,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# ê°œì„  í­ ê³„ì‚°\n",
    "baseline_f1 = results_df.iloc[0]['F1 Score']\n",
    "baseline_auc = results_df.iloc[0]['AUC']\n",
    "\n",
    "results_df['Î”F1'] = results_df['F1 Score'] - baseline_f1\n",
    "results_df['Î”AUC'] = results_df['AUC'] - baseline_auc\n",
    "results_df['F1 ì¦ê°€ìœ¨ (%)'] = (results_df['Î”F1'] / baseline_f1 * 100).round(1)\n",
    "results_df['AUC ì¦ê°€ìœ¨ (%)'] = (results_df['Î”AUC'] / baseline_auc * 100).round(1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ğŸ“Š ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼\")\n",
    "print(\"=\"*100)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36121305",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Feature Importance ë¶„ì„ (ì–´ë–¤ í”¼ì²˜ê°€ ê¸°ì—¬í–ˆëŠ”ì§€)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0a8d4f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“Š Feature Importance (ìµœì¢… ëª¨ë¸: Baseline + ì‹œê³„ì—´ + ê³ ê°ì ‘ì )\n",
      "================================================================================\n",
      "                 feature  importance   íƒ€ì…\n",
      "   payment_failure_count    0.126662 ê³ ê°ì ‘ì \n",
      "   freq_of_use_trend_14d    0.110627  ì‹œê³„ì—´\n",
      " listening_time_trend_7d    0.107120  ì‹œê³„ì—´\n",
      "     app_crash_count_30d    0.085796 ê³ ê°ì ‘ì \n",
      "   skip_rate_increase_7d    0.066356  ì‹œê³„ì—´\n",
      "   days_since_last_login    0.053348  ì‹œê³„ì—´\n",
      "     login_frequency_30d    0.052325  ì‹œê³„ì—´\n",
      "        engagement_score    0.049644   ê¸°ì¡´\n",
      "          skip_intensity    0.049575   ê¸°ì¡´\n",
      "          listening_time    0.048937   ê¸°ì¡´\n",
      "        songs_per_minute    0.044600   ê¸°ì¡´\n",
      "                     age    0.044314   ê¸°ì¡´\n",
      "    songs_played_per_day    0.043370   ê¸°ì¡´\n",
      "               skip_rate    0.035551   ê¸°ì¡´\n",
      "           skip_rate_cap    0.034112   ê¸°ì¡´\n",
      "            ads_pressure    0.015635   ê¸°ì¡´\n",
      "   ads_listened_per_week    0.015262   ê¸°ì¡´\n",
      " promotional_email_click    0.007095 ê³ ê°ì ‘ì \n",
      "customer_support_contact    0.006179 ê³ ê°ì ‘ì \n",
      "       offline_listening    0.003493   ê¸°ì¡´\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 6-1. ìµœì¢… ëª¨ë¸ (ì‹œë‚˜ë¦¬ì˜¤ 3)ì˜ Feature Importance\n",
    "\n",
    "X_final = df[features_scenario3]\n",
    "y_final = df['is_churned']\n",
    "\n",
    "X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n",
    "    X_final, y_final, test_size=0.15, random_state=42, stratify=y_final\n",
    ")\n",
    "\n",
    "rf_final = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_split=5,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "rf_final.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Feature Importance ì¶”ì¶œ\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': features_scenario3,\n",
    "    'importance': rf_final.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# í”¼ì²˜ íƒ€ì… êµ¬ë¶„\n",
    "def categorize_feature(feat):\n",
    "    if feat in TIMESERIES_COLS:\n",
    "        return 'ì‹œê³„ì—´'\n",
    "    elif feat in CUSTOMER_CONTACT_COLS:\n",
    "        return 'ê³ ê°ì ‘ì '\n",
    "    else:\n",
    "        return 'ê¸°ì¡´'\n",
    "\n",
    "importance_df['íƒ€ì…'] = importance_df['feature'].apply(categorize_feature)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š Feature Importance (ìµœì¢… ëª¨ë¸: Baseline + ì‹œê³„ì—´ + ê³ ê°ì ‘ì )\")\n",
    "print(\"=\"*80)\n",
    "print(importance_df.to_string(index=False))\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c34a0be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“Š í”¼ì²˜ íƒ€ì…ë³„ ì¤‘ìš”ë„ ìš”ì•½\n",
      "============================================================\n",
      "           sum      mean  count\n",
      "íƒ€ì…                             \n",
      "ì‹œê³„ì—´   0.389775  0.077955      5\n",
      "ê¸°ì¡´    0.384493  0.034954     11\n",
      "ê³ ê°ì ‘ì   0.225732  0.056433      4\n",
      "============================================================\n",
      "\n",
      "ğŸ’¡ í•´ì„:\n",
      "  - ì‹œê³„ì—´ í”¼ì²˜ì˜ ì¤‘ìš”ë„ í•©ê³„ê°€ ë†’ë‹¤ë©´ â†’ í–‰ë™ ë³€í™” ì¶”ì ì´ í•µì‹¬\n",
      "  - ê³ ê°ì ‘ì  í”¼ì²˜ì˜ ì¤‘ìš”ë„ê°€ ë†’ë‹¤ë©´ â†’ ë¶ˆë§Œ/ë¬¸ì œ ì‹ í˜¸ê°€ ì¤‘ìš”\n",
      "  - ê¸°ì¡´ í”¼ì²˜ê°€ ì—¬ì „íˆ ë†’ë‹¤ë©´ â†’ ê¸°ë³¸ í–‰ë™ íŒ¨í„´ë„ ì¤‘ìš”\n"
     ]
    }
   ],
   "source": [
    "# 6-2. í”¼ì²˜ íƒ€ì…ë³„ ì¤‘ìš”ë„ í•©ê³„\n",
    "\n",
    "importance_by_type = importance_df.groupby('íƒ€ì…')['importance'].agg(['sum', 'mean', 'count'])\n",
    "importance_by_type = importance_by_type.sort_values('sum', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š í”¼ì²˜ íƒ€ì…ë³„ ì¤‘ìš”ë„ ìš”ì•½\")\n",
    "print(\"=\"*60)\n",
    "print(importance_by_type)\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nğŸ’¡ í•´ì„:\")\n",
    "print(\"  - ì‹œê³„ì—´ í”¼ì²˜ì˜ ì¤‘ìš”ë„ í•©ê³„ê°€ ë†’ë‹¤ë©´ â†’ í–‰ë™ ë³€í™” ì¶”ì ì´ í•µì‹¬\")\n",
    "print(\"  - ê³ ê°ì ‘ì  í”¼ì²˜ì˜ ì¤‘ìš”ë„ê°€ ë†’ë‹¤ë©´ â†’ ë¶ˆë§Œ/ë¬¸ì œ ì‹ í˜¸ê°€ ì¤‘ìš”\")\n",
    "print(\"  - ê¸°ì¡´ í”¼ì²˜ê°€ ì—¬ì „íˆ ë†’ë‹¤ë©´ â†’ ê¸°ë³¸ í–‰ë™ íŒ¨í„´ë„ ì¤‘ìš”\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b247ed59",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. ìµœì¢… ê²°ê³¼ í•´ì„ ë° ì œì•ˆ ê²€ì¦\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "029904aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ğŸ¯ ìµœì¢… ìš”ì•½ ë° ê²°ë¡ \n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“Š ì„±ëŠ¥ ê°œì„  ê²°ê³¼:\n",
      "  Baseline  : F1 0.4153, AUC 0.5352 (11ê°œ í”¼ì²˜ - Set D)\n",
      "  Advanced  : F1 0.6246, AUC 0.7938 (20ê°œ í”¼ì²˜)\n",
      "  ê°œì„  í­   : Î”F1 +0.2093 (+50.4%), Î”AUC +0.2587 (+48.3%)\n",
      "\n",
      "ğŸ’¡ í•µì‹¬ ë°œê²¬:\n",
      "  âœ… ì‹œê³„ì—´ + ê³ ê°ì ‘ì  í”¼ì²˜ ì¶”ê°€ ì‹œ **ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ í–¥ìƒ** (Î”F1 > 0.05)\n",
      "  âœ… improvement_proposal.mdì˜ ì œì•ˆì´ **ê²€ì¦ë¨**\n",
      "  âœ… ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ì‹œ F1 0.6+, AUC 0.7+ ë‹¬ì„± ê°€ëŠ¥ì„± ë†’ìŒ\n",
      "\n",
      "ğŸ¯ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€:\n",
      "  âœ… ëª©í‘œ ë‹¬ì„±! (F1 0.6+, AUC 0.7+)\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 7-1. ìµœì¢… ìš”ì•½\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ğŸ¯ ìµœì¢… ìš”ì•½ ë° ê²°ë¡ \")\n",
    "print(\"=\"*100)\n",
    "\n",
    "final_f1 = results_df.iloc[-1]['F1 Score']\n",
    "final_auc = results_df.iloc[-1]['AUC']\n",
    "delta_f1 = results_df.iloc[-1]['Î”F1']\n",
    "delta_auc = results_df.iloc[-1]['Î”AUC']\n",
    "f1_increase_pct = results_df.iloc[-1]['F1 ì¦ê°€ìœ¨ (%)']\n",
    "auc_increase_pct = results_df.iloc[-1]['AUC ì¦ê°€ìœ¨ (%)']\n",
    "\n",
    "print(f\"\\nğŸ“Š ì„±ëŠ¥ ê°œì„  ê²°ê³¼:\")\n",
    "print(f\"  Baseline  : F1 {baseline_f1:.4f}, AUC {baseline_auc:.4f} (11ê°œ í”¼ì²˜ - Set D)\")\n",
    "print(f\"  Advanced  : F1 {final_f1:.4f}, AUC {final_auc:.4f} (20ê°œ í”¼ì²˜)\")\n",
    "print(f\"  ê°œì„  í­   : Î”F1 {delta_f1:+.4f} ({f1_increase_pct:+.1f}%), Î”AUC {delta_auc:+.4f} ({auc_increase_pct:+.1f}%)\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ í•µì‹¬ ë°œê²¬:\")\n",
    "\n",
    "if delta_f1 > 0.05:\n",
    "    print(f\"  âœ… ì‹œê³„ì—´ + ê³ ê°ì ‘ì  í”¼ì²˜ ì¶”ê°€ ì‹œ **ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ í–¥ìƒ** (Î”F1 > 0.05)\")\n",
    "    print(f\"  âœ… improvement_proposal.mdì˜ ì œì•ˆì´ **ê²€ì¦ë¨**\")\n",
    "    print(f\"  âœ… ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ì‹œ F1 0.6+, AUC 0.7+ ë‹¬ì„± ê°€ëŠ¥ì„± ë†’ìŒ\")\n",
    "elif delta_f1 > 0.02:\n",
    "    print(f\"  âš ï¸  ì¤‘ê°„ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ í–¥ìƒ (Î”F1 = {delta_f1:.4f})\")\n",
    "    print(f\"  âš ï¸  ì‹¤ì œ ë°ì´í„°ì˜ íŒ¨í„´ì´ ë” ëª…í™•í•˜ë‹¤ë©´ ì¶”ê°€ í–¥ìƒ ê°€ëŠ¥\")\n",
    "else:\n",
    "    print(f\"  âš ï¸  ì„±ëŠ¥ í–¥ìƒì´ ì œí•œì  (Î”F1 = {delta_f1:.4f})\")\n",
    "    print(f\"  âš ï¸  í•©ì„± ë°ì´í„°ì˜ íŒ¨í„´ì´ ì‹¤ì œì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€:\")\n",
    "target_f1 = 0.60\n",
    "target_auc = 0.70\n",
    "\n",
    "if final_f1 >= target_f1 and final_auc >= target_auc:\n",
    "    print(f\"  âœ… ëª©í‘œ ë‹¬ì„±! (F1 {target_f1}+, AUC {target_auc}+)\")\n",
    "elif final_f1 >= 0.50 and final_auc >= 0.62:\n",
    "    print(f\"  ğŸŸ¡ ì¤‘ê°„ ëª©í‘œ ë‹¬ì„± (F1 0.50+, AUC 0.62+)\")\n",
    "    print(f\"  ğŸŸ¡ ì¶”ê°€ í”¼ì²˜(ì½˜í…ì¸  ì†Œë¹„ íŒ¨í„´ ë“±)ë¡œ ìµœì¢… ëª©í‘œ ë‹¬ì„± ê°€ëŠ¥\")\n",
    "else:\n",
    "    print(f\"  ğŸ”´ ëª©í‘œ ë¯¸ë‹¬ (í˜„ì¬ F1 {final_f1:.4f}, AUC {final_auc:.4f})\")\n",
    "    print(f\"  ğŸ”´ ë” ê°•ë ¥í•œ ì‹ í˜¸ë¥¼ ê°€ì§„ í”¼ì²˜ í•„ìš”\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8dbaea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Enhanced Data ì €ì¥ ë° ìµœì¢… ì„±ëŠ¥ ê²€ì¦\n",
    "\n",
    "ì €ì¥ëœ `enhanced_data.csv`ë¥¼ ë‹¤ì‹œ ë¡œë“œí•˜ì—¬ ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ì„ ê²€ì¦í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8f6d5389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“ Enhanced Data ì €ì¥\n",
      "================================================================================\n",
      "\n",
      "ì €ì¥í•  ë°ì´í„°:\n",
      "  - í–‰ ê°œìˆ˜: 8,000ê°œ\n",
      "  - ì „ì²´ ì»¬ëŸ¼: 26ê°œ\n",
      "  - íŒŒì¼ ê²½ë¡œ: ../data/enhanced_data.csv\n",
      "\n",
      "âœ… ì €ì¥ ì™„ë£Œ!\n",
      "\n",
      "í¬í•¨ëœ í”¼ì²˜ ê·¸ë£¹:\n",
      "  - ê¸°ë³¸ ìˆ˜ì¹˜í˜•: 6ê°œ\n",
      "  - FE í”¼ì²˜: 5ê°œ\n",
      "  - ì‹œê³„ì—´ í”¼ì²˜: 5ê°œ\n",
      "  - ê³ ê°ì ‘ì  í”¼ì²˜: 4ê°œ\n",
      "  - ê¸°íƒ€ (ë²”ì£¼í˜• ë“±): 6ê°œ\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 8-1. Enhanced Data CSV ì €ì¥\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“ Enhanced Data ì €ì¥\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ì €ì¥í•  ë°ì´í„° í™•ì¸\n",
    "print(f\"\\nì €ì¥í•  ë°ì´í„°:\")\n",
    "print(f\"  - í–‰ ê°œìˆ˜: {len(df):,}ê°œ\")\n",
    "print(f\"  - ì „ì²´ ì»¬ëŸ¼: {len(df.columns)}ê°œ\")\n",
    "print(f\"  - íŒŒì¼ ê²½ë¡œ: ../data/enhanced_data.csv\")\n",
    "\n",
    "# CSV ì €ì¥\n",
    "output_path = '../data/enhanced_data.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… ì €ì¥ ì™„ë£Œ!\")\n",
    "print(f\"\\ní¬í•¨ëœ í”¼ì²˜ ê·¸ë£¹:\")\n",
    "print(f\"  - ê¸°ë³¸ ìˆ˜ì¹˜í˜•: {len(BASE_NUM_COLS)}ê°œ\")\n",
    "print(f\"  - FE í”¼ì²˜: {len(BASE_FE_COLS)}ê°œ\")\n",
    "print(f\"  - ì‹œê³„ì—´ í”¼ì²˜: {len(TIMESERIES_COLS)}ê°œ\")\n",
    "print(f\"  - ê³ ê°ì ‘ì  í”¼ì²˜: {len(CUSTOMER_CONTACT_COLS)}ê°œ\")\n",
    "print(f\"  - ê¸°íƒ€ (ë²”ì£¼í˜• ë“±): {len(df.columns) - len(BASE_NUM_COLS) - len(BASE_FE_COLS) - len(TIMESERIES_COLS) - len(CUSTOMER_CONTACT_COLS)}ê°œ\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9d1dd5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ”„ Enhanced Data ì¬ë¡œë“œ ë° ê²€ì¦\n",
      "================================================================================\n",
      "\n",
      "ë¡œë“œëœ ë°ì´í„°:\n",
      "  - í–‰ ê°œìˆ˜: 8,000ê°œ\n",
      "  - ì»¬ëŸ¼ ê°œìˆ˜: 26ê°œ\n",
      "  - ì´íƒˆë¥ : 25.89%\n",
      "\n",
      "âœ… ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ì™„ë£Œ!\n",
      "\n",
      "ì»¬ëŸ¼ ëª©ë¡ (ì²˜ìŒ 10ê°œ):\n",
      "   1. user_id\n",
      "   2. gender\n",
      "   3. age\n",
      "   4. country\n",
      "   5. subscription_type\n",
      "   6. listening_time\n",
      "   7. songs_played_per_day\n",
      "   8. skip_rate\n",
      "   9. device_type\n",
      "  10. ads_listened_per_week\n",
      "  ... (ì´ 26ê°œ)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 8-2. Enhanced Data ì¬ë¡œë“œ ë° ê²€ì¦\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ”„ Enhanced Data ì¬ë¡œë“œ ë° ê²€ì¦\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# CSV ì¬ë¡œë“œ\n",
    "df_reload = pd.read_csv('../data/enhanced_data.csv')\n",
    "\n",
    "print(f\"\\në¡œë“œëœ ë°ì´í„°:\")\n",
    "print(f\"  - í–‰ ê°œìˆ˜: {len(df_reload):,}ê°œ\")\n",
    "print(f\"  - ì»¬ëŸ¼ ê°œìˆ˜: {len(df_reload.columns)}ê°œ\")\n",
    "print(f\"  - ì´íƒˆë¥ : {df_reload['is_churned'].mean():.2%}\")\n",
    "\n",
    "# ë°ì´í„° ì¼ì¹˜ í™•ì¸\n",
    "assert len(df) == len(df_reload), \"âŒ í–‰ ê°œìˆ˜ ë¶ˆì¼ì¹˜!\"\n",
    "assert len(df.columns) == len(df_reload.columns), \"âŒ ì»¬ëŸ¼ ê°œìˆ˜ ë¶ˆì¼ì¹˜!\"\n",
    "\n",
    "print(f\"\\nâœ… ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ì™„ë£Œ!\")\n",
    "print(f\"\\nì»¬ëŸ¼ ëª©ë¡ (ì²˜ìŒ 10ê°œ):\")\n",
    "for i, col in enumerate(df_reload.columns[:10], 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "if len(df_reload.columns) > 10:\n",
    "    print(f\"  ... (ì´ {len(df_reload.columns)}ê°œ)\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3fc9d3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ¤– ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (RandomForest)\n",
      "================================================================================\n",
      "\n",
      "ì‚¬ìš©í•  í”¼ì²˜: 20ê°œ\n",
      "  - Baseline: 11ê°œ\n",
      "  - ì‹œê³„ì—´: 5ê°œ\n",
      "  - ê³ ê°ì ‘ì : 4ê°œ\n",
      "\n",
      "ë°ì´í„° ë¶„í• :\n",
      "  - Train: 6,400ê°œ (80%)\n",
      "  - Test:  1,600ê°œ (20%)\n",
      "\n",
      "ğŸ”§ ëª¨ë¸ í•™ìŠµ ì¤‘...\n",
      "âœ… í•™ìŠµ ì™„ë£Œ!\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š ìµœì¢… ì„±ëŠ¥ ê²°ê³¼\n",
      "================================================================================\n",
      "  F1 Score       : 0.6205\n",
      "  AUC            : 0.7863\n",
      "  Best Threshold : 0.32\n",
      "  í”¼ì²˜ ê°œìˆ˜      : 20ê°œ\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¡ Baseline ëŒ€ë¹„ ê°œì„ :\n",
      "  Î”F1  : +0.2052 (+49.4%)\n",
      "  Î”AUC : +0.2511 (+46.9%)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 8-3. Enhanced Dataë¡œ ìµœì¢… RandomForest ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ¤– ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (RandomForest)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# í”¼ì²˜ ì •ì˜ (20ê°œ: ê¸°ì¡´ 11 + ì‹œê³„ì—´ 5 + ê³ ê°ì ‘ì  4)\n",
    "final_features = BASELINE_FEATURES + TIMESERIES_COLS + CUSTOMER_CONTACT_COLS\n",
    "\n",
    "print(f\"\\nì‚¬ìš©í•  í”¼ì²˜: {len(final_features)}ê°œ\")\n",
    "print(f\"  - Baseline: {len(BASELINE_FEATURES)}ê°œ\")\n",
    "print(f\"  - ì‹œê³„ì—´: {len(TIMESERIES_COLS)}ê°œ\")\n",
    "print(f\"  - ê³ ê°ì ‘ì : {len(CUSTOMER_CONTACT_COLS)}ê°œ\")\n",
    "\n",
    "# ë°ì´í„° ì¤€ë¹„\n",
    "X_final = df_reload[final_features]\n",
    "y_final = df_reload['is_churned']\n",
    "\n",
    "# Train/Test Split\n",
    "X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n",
    "    X_final, y_final, test_size=0.2, random_state=42, stratify=y_final\n",
    ")\n",
    "\n",
    "print(f\"\\në°ì´í„° ë¶„í• :\")\n",
    "print(f\"  - Train: {len(X_train_final):,}ê°œ (80%)\")\n",
    "print(f\"  - Test:  {len(X_test_final):,}ê°œ (20%)\")\n",
    "\n",
    "# RandomForest ëª¨ë¸ í•™ìŠµ\n",
    "print(f\"\\nğŸ”§ ëª¨ë¸ í•™ìŠµ ì¤‘...\")\n",
    "rf_final = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_split=5,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "rf_final.fit(X_train_final, y_train_final)\n",
    "print(f\"âœ… í•™ìŠµ ì™„ë£Œ!\")\n",
    "\n",
    "# ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€\n",
    "y_proba_final = rf_final.predict_proba(X_test_final)[:, 1]\n",
    "auc_final = roc_auc_score(y_test_final, y_proba_final)\n",
    "\n",
    "# ìµœì  threshold íƒìƒ‰\n",
    "thresholds = np.arange(0.05, 0.50, 0.01)\n",
    "best_f1_final = 0\n",
    "best_th_final = 0.1\n",
    "\n",
    "for th in thresholds:\n",
    "    y_pred_th = (y_proba_final >= th).astype(int)\n",
    "    f1_th = f1_score(y_test_final, y_pred_th)\n",
    "    if f1_th > best_f1_final:\n",
    "        best_f1_final = f1_th\n",
    "        best_th_final = th\n",
    "\n",
    "# ìµœì  thresholdë¡œ ìµœì¢… ì˜ˆì¸¡\n",
    "y_pred_final = (y_proba_final >= best_th_final).astype(int)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š ìµœì¢… ì„±ëŠ¥ ê²°ê³¼\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  F1 Score       : {best_f1_final:.4f}\")\n",
    "print(f\"  AUC            : {auc_final:.4f}\")\n",
    "print(f\"  Best Threshold : {best_th_final:.2f}\")\n",
    "print(f\"  í”¼ì²˜ ê°œìˆ˜      : {len(final_features)}ê°œ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Baselineê³¼ ë¹„êµ\n",
    "baseline_f1_ref = 0.4153  # ì´ì „ ê²°ê³¼\n",
    "baseline_auc_ref = 0.5352\n",
    "\n",
    "improvement_f1 = best_f1_final - baseline_f1_ref\n",
    "improvement_auc = auc_final - baseline_auc_ref\n",
    "improvement_f1_pct = (improvement_f1 / baseline_f1_ref) * 100\n",
    "improvement_auc_pct = (improvement_auc / baseline_auc_ref) * 100\n",
    "\n",
    "print(f\"\\nğŸ’¡ Baseline ëŒ€ë¹„ ê°œì„ :\")\n",
    "print(f\"  Î”F1  : {improvement_f1:+.4f} ({improvement_f1_pct:+.1f}%)\")\n",
    "print(f\"  Î”AUC : {improvement_auc:+.4f} ({improvement_auc_pct:+.1f}%)\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "17ab3809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“‹ ìƒì„¸ ì„±ëŠ¥ ë¶„ì„\n",
      "================================================================================\n",
      "\n",
      "í˜¼ë™ í–‰ë ¬ (Confusion Matrix):\n",
      "                 ì˜ˆì¸¡: ë¹„ì´íƒˆ  ì˜ˆì¸¡: ì´íƒˆ\n",
      "  ì‹¤ì œ: ë¹„ì´íƒˆ      1042          144\n",
      "  ì‹¤ì œ: ì´íƒˆ         163          251\n",
      "\n",
      "ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ë¹„ì´íƒˆ     0.8647    0.8786    0.8716      1186\n",
      "          ì´íƒˆ     0.6354    0.6063    0.6205       414\n",
      "\n",
      "    accuracy                         0.8081      1600\n",
      "   macro avg     0.7501    0.7424    0.7461      1600\n",
      "weighted avg     0.8054    0.8081    0.8066      1600\n",
      "\n",
      "ì¶”ê°€ ì„±ëŠ¥ ì§€í‘œ:\n",
      "  - Accuracy  : 0.8081\n",
      "  - Precision : 0.6354\n",
      "  - Recall    : 0.6063\n",
      "  - F1 Score  : 0.6205\n",
      "  - AUC       : 0.7863\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 8-4. Confusion Matrix ë° Classification Report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“‹ ìƒì„¸ ì„±ëŠ¥ ë¶„ì„\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test_final, y_pred_final)\n",
    "print(f\"\\ní˜¼ë™ í–‰ë ¬ (Confusion Matrix):\")\n",
    "print(f\"                 ì˜ˆì¸¡: ë¹„ì´íƒˆ  ì˜ˆì¸¡: ì´íƒˆ\")\n",
    "print(f\"  ì‹¤ì œ: ë¹„ì´íƒˆ    {cm[0,0]:6d}       {cm[0,1]:6d}\")\n",
    "print(f\"  ì‹¤ì œ: ì´íƒˆ      {cm[1,0]:6d}       {cm[1,1]:6d}\")\n",
    "\n",
    "# Classification Report\n",
    "print(f\"\\në¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(y_test_final, y_pred_final, \n",
    "                          target_names=['ë¹„ì´íƒˆ', 'ì´íƒˆ'], \n",
    "                          digits=4))\n",
    "\n",
    "# ì¶”ê°€ ì§€í‘œ\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "precision = precision_score(y_test_final, y_pred_final)\n",
    "recall = recall_score(y_test_final, y_pred_final)\n",
    "accuracy = accuracy_score(y_test_final, y_pred_final)\n",
    "\n",
    "print(f\"ì¶”ê°€ ì„±ëŠ¥ ì§€í‘œ:\")\n",
    "print(f\"  - Accuracy  : {accuracy:.4f}\")\n",
    "print(f\"  - Precision : {precision:.4f}\")\n",
    "print(f\"  - Recall    : {recall:.4f}\")\n",
    "print(f\"  - F1 Score  : {best_f1_final:.4f}\")\n",
    "print(f\"  - AUC       : {auc_final:.4f}\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a882c6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ğŸ¯ ìµœì¢… ìš”ì•½\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“ ìƒì„±ëœ íŒŒì¼:\n",
      "  - enhanced_data.csv (8,000í–‰ Ã— 26ì—´)\n",
      "\n",
      "ğŸ“Š ìµœì¢… ëª¨ë¸ ì„±ëŠ¥:\n",
      "  - RandomForest (n_estimators=300, class_weight='balanced')\n",
      "  - Train/Test Split: 80% / 20%\n",
      "  - í”¼ì²˜ ê°œìˆ˜: 20ê°œ\n",
      "\n",
      "  ì„±ëŠ¥ ì§€í‘œ:\n",
      "    â”œâ”€ F1 Score : 0.6205\n",
      "    â”œâ”€ AUC      : 0.7863\n",
      "    â”œâ”€ Precision: 0.6354\n",
      "    â”œâ”€ Recall   : 0.6063\n",
      "    â””â”€ Accuracy : 0.8081\n",
      "\n",
      "ğŸ’¡ Baseline ëŒ€ë¹„ ê°œì„ :\n",
      "    â”œâ”€ Î”F1  : +0.2052 (+49.4%)\n",
      "    â””â”€ Î”AUC : +0.2511 (+46.9%)\n",
      "\n",
      "âœ… í•µì‹¬ ì„±ê³¼:\n",
      "  ğŸŸ¡ ì¤‘ê°„ ëª©í‘œ ë‹¬ì„± (F1 0.60+, AUC 0.70+)\n",
      "  ğŸŸ¡ ì¶”ê°€ íŠœë‹ìœ¼ë¡œ ìµœì¢… ëª©í‘œ ë‹¬ì„± ê°€ëŠ¥\n",
      "\n",
      "ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:\n",
      "  1. enhanced_data.csvë¥¼ í™œìš©í•œ ë‹¤ë¥¸ ëª¨ë¸ ì‹¤í—˜ (XGBoost, LightGBM ë“±)\n",
      "  2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ìœ¼ë¡œ ì¶”ê°€ ì„±ëŠ¥ ê°œì„ \n",
      "  3. ë°œí‘œ ìë£Œ ì‘ì„± ë° ê²°ê³¼ ì •ë¦¬\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 8-5. ìµœì¢… ìš”ì•½ ë° ê²°ë¡ \n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ğŸ¯ ìµœì¢… ìš”ì•½\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\nğŸ“ ìƒì„±ëœ íŒŒì¼:\")\n",
    "print(f\"  - enhanced_data.csv ({len(df_reload):,}í–‰ Ã— {len(df_reload.columns)}ì—´)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š ìµœì¢… ëª¨ë¸ ì„±ëŠ¥:\")\n",
    "print(f\"  - RandomForest (n_estimators=300, class_weight='balanced')\")\n",
    "print(f\"  - Train/Test Split: 80% / 20%\")\n",
    "print(f\"  - í”¼ì²˜ ê°œìˆ˜: {len(final_features)}ê°œ\")\n",
    "print(f\"\")\n",
    "print(f\"  ì„±ëŠ¥ ì§€í‘œ:\")\n",
    "print(f\"    â”œâ”€ F1 Score : {best_f1_final:.4f}\")\n",
    "print(f\"    â”œâ”€ AUC      : {auc_final:.4f}\")\n",
    "print(f\"    â”œâ”€ Precision: {precision:.4f}\")\n",
    "print(f\"    â”œâ”€ Recall   : {recall:.4f}\")\n",
    "print(f\"    â””â”€ Accuracy : {accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Baseline ëŒ€ë¹„ ê°œì„ :\")\n",
    "print(f\"    â”œâ”€ Î”F1  : {improvement_f1:+.4f} ({improvement_f1_pct:+.1f}%)\")\n",
    "print(f\"    â””â”€ Î”AUC : {improvement_auc:+.4f} ({improvement_auc_pct:+.1f}%)\")\n",
    "\n",
    "print(f\"\\nâœ… í•µì‹¬ ì„±ê³¼:\")\n",
    "if best_f1_final >= 0.65 and auc_final >= 0.75:\n",
    "    print(f\"  âœ… ëª©í‘œ ë‹¬ì„±! (F1 0.65+, AUC 0.75+)\")\n",
    "    print(f\"  âœ… ì‹œê³„ì—´ + ê³ ê°ì ‘ì  í”¼ì²˜ ì¶”ê°€ë¡œ ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ í–¥ìƒ\")\n",
    "    print(f\"  âœ… improvement_proposal.mdì˜ ì œì•ˆ ê²€ì¦ ì™„ë£Œ\")\n",
    "elif best_f1_final >= 0.60 and auc_final >= 0.70:\n",
    "    print(f\"  ğŸŸ¡ ì¤‘ê°„ ëª©í‘œ ë‹¬ì„± (F1 0.60+, AUC 0.70+)\")\n",
    "    print(f\"  ğŸŸ¡ ì¶”ê°€ íŠœë‹ìœ¼ë¡œ ìµœì¢… ëª©í‘œ ë‹¬ì„± ê°€ëŠ¥\")\n",
    "else:\n",
    "    print(f\"  âš ï¸  ì„±ëŠ¥ í–¥ìƒ í™•ì¸ë˜ë‚˜ ì¶”ê°€ ê°œì„  í•„ìš”\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(f\"  1. enhanced_data.csvë¥¼ í™œìš©í•œ ë‹¤ë¥¸ ëª¨ë¸ ì‹¤í—˜ (XGBoost, LightGBM ë“±)\")\n",
    "print(f\"  2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ìœ¼ë¡œ ì¶”ê°€ ì„±ëŠ¥ ê°œì„ \")\n",
    "print(f\"  3. ë°œí‘œ ìë£Œ ì‘ì„± ë° ê²°ê³¼ ì •ë¦¬\")\n",
    "\n",
    "print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a474b24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í•©ì„± í”¼ì²˜ í¬í•¨ ë°ì´í„° ì €ì¥ ì™„ë£Œ: ../data/enhanced_data.csv\n",
      "   ì´ í”¼ì²˜ ê°œìˆ˜: 26ê°œ\n",
      "   ë°ì´í„° í¬ê¸°: (8000, 26)\n",
      "âœ… í•©ì„± í”¼ì²˜ë§Œ ì €ì¥ ì™„ë£Œ: ../data/synthetic_features.csv\n"
     ]
    }
   ],
   "source": [
    "# 8. í•©ì„± í”¼ì²˜ í¬í•¨ ë°ì´í„° ì €ì¥\n",
    "\n",
    "# ì˜µì…˜ 1: ì „ì²´ ë°ì´í„° ì €ì¥ (ê¸°ì¡´ + FE + í•©ì„±)\n",
    "output_path = '../data/enhanced_data.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"âœ… í•©ì„± í”¼ì²˜ í¬í•¨ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "print(f\"   ì´ í”¼ì²˜ ê°œìˆ˜: {len(df.columns)}ê°œ\")\n",
    "print(f\"   ë°ì´í„° í¬ê¸°: {df.shape}\")\n",
    "\n",
    "# ì˜µì…˜ 2: í•©ì„± í”¼ì²˜ë§Œ ë”°ë¡œ ì €ì¥ (ì¶”ê°€ìš©)\n",
    "synthetic_cols = TIMESERIES_COLS + CUSTOMER_CONTACT_COLS\n",
    "df[['user_id'] + synthetic_cols].to_csv(\n",
    "    '../data/synthetic_features.csv', index=False\n",
    ")\n",
    "print(f\"âœ… í•©ì„± í”¼ì²˜ë§Œ ì €ì¥ ì™„ë£Œ: ../data/synthetic_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ce241f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
